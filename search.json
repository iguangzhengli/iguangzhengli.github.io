[{"title":"MySQL语句执行细节","url":"/2020/08/01/MySQL实现细节/","content":"\n# MySQL实现细节\n\n## delete\n\n### 删除数据流程\n\nInnoDB 里的数据都是用 B+ 树的结构组织的。ID为主键的聚簇索引。如果我们用 delete 命令把整个表的数据删除呢?结果就是，所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。\n\n你现在知道了，delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。\n\n实际上，不止是删除数据会造成空洞，插入数据也会。 如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。\n\n<!-- more -->\n\n### 重建表\n\n可以使用 alter table A engine=InnoDB 命令来重建表。\n\nMySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。 我给你简单描述一下引入了 Online DDL 之后，重建表的流程:\n\n1. 建立一个临时文件，扫描表 A 主键的所有数据页;\n\n2. 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中;\n\n3. 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件(row log)中;\n\n4. 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件;\n\n5. 用临时文件替换表 A 的数据文件。\n\n由于日志文件记录和重放操作这个功能的存在， 这个方案在重建表的过程中，允许对表 A 做增删改操作。这也就是 Online DDL 名字的来源。\n\n对于很大的 表来说，这个操作是很消耗 IO 和 CPU 资源的。因此，如果是线上服务，你要很小心地控制操作时间。如果想要比较安全的操作的话，我推荐你使用 GitHub 开源的 gh-ost 来 做。\n\n## count(*)\n\n在不同的 MySQL 引擎中，count(*) 有不同的实现方式。\n\nMyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高;\n\n而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面 读出来，然后累积计数。\n\n这和 InnoDB 的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是 MVCC 来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。\n\n你知道的，InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。\n\n假设我创建一张表\n\n```\nCREATE TABLE `metrics_class`  (\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `package_id` bigint(20) NOT NULL,\n  `class_name` varchar(255) NOT NULL,\n  PRIMARY KEY (`id`),\n  INDEX `idx_package_id` (`package_id`)\n) ENGINE=InnoDB;\n```\n\n```\nEXPLAIN SELECT COUNT(1) FROM metrics_class;\n```\n\n会发现命中了 `idx_package_id` 这个索引，这是因为这个索引数比主键索引的聚簇索引要小，mysql会遍历这棵树。\n\n### count(*) count(1) count(id) count(field)\n\n对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。\n\n对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。\n\n单看这两个用法的差别的话，你能对比出来，count(1) 执行得要比 count(主键 id) 快。因 为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。\n\n对于 count(字段) 来说:\n\n1. 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加;\n\n2. 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加\n\n但是 count(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。 count(*) 肯定不是 null，按行累加。\n\ncount(字段)<count(主键 id)<count(1)≈count(*)\n\n## order\n\n创建表\n\n```\nCREATE TABLE `t` (\n`id` int(11) NOT NULL,\n`city` varchar(16) NOT NULL,\n`name` varchar(16) NOT NULL,\n`age` int(11) NOT NULL,\n`addr` varchar(128) DEFAULT NULL, \nPRIMARY KEY (`id`),\nKEY `city` (`city`)\n)ENGINE=InnoDB;\n```\n\n### 全字段排序\n\n查询\n\n```\nselect city,name,age from t where city='杭州' order by name limit 1000 ;\n```\n\n为避免全表扫描，我们需要在 city 字段加 上索引。在 city 字段上创建索引之后，我们用 explain 命令来看看这个语句的执行情况。\n\n![GbuWgz](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/GbuWgz.png)\n\nExtra 这个字段中的“Using filesort”表示的就是需要排序，MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。\n\n这个语句执行流程如下所示 :\n\n1. 初始化 sort_buffer，确定放入 name、city、age 这三个字段;\n\n2. 从索引 city 找到第一个满足 city='杭州’条件的主键 id，也就是图中的 ID_X;\n\n3. 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中;\n\n4. 从索引 city 取下一个记录的主键 id;\n\n5. 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的ID_Y;\n\n6. 对 sort_buffer 中的数据按照字段 name 做快速排序;\n\n7. 按照排序结果取前 1000 行返回给客户端。\n\n我们暂且把这个排序过程，称为全字段排序。步骤6中“按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决 于排序所需的内存和参数 sort_buffer_size。\n\nsort_buffer_size，就是 MySQL 为排序开辟的内存(sort_buffer)的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。内存放不下时，就需要使用外部排序，外部排序一般使用归并排序算法。 可以这么简单理解，MySQL 将需要排序的数据分成 12 份，每一份单独排序后存在这些临时文件中。然后把这 12 个有序文件再合并成一个有序的大文件。\n\n### rowed 排序\n\n在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。\n\n所以如果单行很大，这个方法效率不够好。那么，如果 MySQL 认为排序的单行长度太大会怎么做呢?\n\n`SET max_length_for_sort_data = 16;`\n\n命令表示单行的长度超过这个16，MySQL 就认为单行太大，要换一个算法，即rowid排序。\n\n新的算法放入 sort_buffer 的字段，只有要排序的列(即 name 字段)和主键 id。\n\n但这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回了，整个执行流程就变成如下所示的样子:\n\n1. 初始化 sort_buffer，确定放入两个字段，即 name 和 id;\n\n2. 从索引 city 找到第一个满足 city='杭州’条件的主键 id，也就是图中的 ID_X;\n\n3. 到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中;\n\n4. 从索引 city 取下一个记录的主键 id;\n\n5. 重复步骤 3、4 直到不满足 city='杭州’条件为止，也就是图中的 ID_Y;\n\n6. 对 sort_buffer 中的数据按照字段 name 进行排序;\n\n7. 遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回。\n\n### 优化\n\n其实，并不是所有的 order by 语句，都需要排序操作的。从上面分析的执行过程，我们 可以看到，MySQL 之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。\n\n你可以设想下，如果能够保证从 city 这个索引上取出来的行，天然就是按照 name 递增排序的话，是不是就可以不用再排序了呢?\n\n所以，我们可以在这个市民表上创建一个 city 和 name 的联合索引，对应的 SQL 语句 是:\n\n`alter table t add index city_user(city, name);`\n\n然后步骤就变为\n\n1. 从索引 (city,name) 找到第一个满足 city='杭州’条件的主键 id;\n\n2. 到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回;\n\n3. 从索引 (city,name) 取下一个记录主键 id;\n\n4. 重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city='杭州’条件时循环结束。\n\n用explain会发现 Extra 字段中没有了 Using filesort了。不需要排序。这个查询也不用把 4000 行全都读一遍，只要找 到满足条件的前 1000 条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描 1000 次。\n\n如果你还想要优化，不想要回表的话，可以用覆盖索引\n\n```\nalter table t add index city_user_age(city, name, age);\n```\n\n不过这个就要自己去考虑，毕竟索引还是有维护代价的。这是一个需要权衡的决定。\n\n### order by range\n\n我在这个表里面插入了 10000 行记录。接下来，我们就一起看看要随机选择 3 个单词，最简单的就是`select word from words order by rand() limit 3;`这条语句的执行流程是这样的:\n\n1. 创建一个临时表。这个临时表使用的是 memory 引擎，表里有两个字段，第一个字段 是 double 类型，为了后面描述方便，记为字段 R，第二个字段是 varchar(64) 类型，记为字段 W。并且，这个表没有建索引。\n\n2. 从 words 表中，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand()函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的R 和 W 字段中，到此，扫描行数是 10000。\n\n3. 现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。\n\n4. 初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。\n\n5. 从内存临时表中一行一行地取出 R 值和位置信息(我后面会和你解释这里为什么是“位置信息”)，分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。\n\n6. 在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。\n\n7. 排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。\n\n接下来，我们通过慢查询日志(slow log)来验证一下我们分析得到的扫描行数是否正确。\n\n```\n# Query_time: 0.900376 Lock_time: 0.000347 Rows_sent: 3 Rows_examined: 20003\nSET timestamp=1541402277;\nselect word from words order by rand() limit 3;\n```\n\n其中，Rows_examined:20003 就表示这个语句执行过程中扫描了 20003 行，也就验证了我们分析得出的结论。\n\n#### 优化\n\n1. 取得整个表的行数，记为 C;\n\n2. 根据相同的随机方法得到 Y1、Y2、Y3; \n\n3. 再执行三个 limit Y, 1 语句得到三行数据。\n\n```\n1 mysql> select count(*) into @C from t;\n2 set @Y1 = floor(@C * rand());\n3 set @Y2 = floor(@C * rand());\n4 set @Y3 = floor(@C * rand());\n5 select * from t limit @Y1，1; // 在应用代码里面取 Y1、Y2、Y3 值，拼出 SQL 后执行\n6 select * from t limit @Y2，1;\n7 select * from t limit @Y3，1;\n```\n\nMySQL 处理 limit Y,1 的做法就是按顺序一个一个地读出来，丢掉前 Y 个，然后把下一个 记录作为返回结果，因此这一步需要扫描 Y+1 行。再加上，第一步扫描的 C 行，总共需 要扫描 C+Y+1 行，执行代价比随机算法 1 的代价要高。\n\n当然=跟直接 order by rand() 比起来，执行代价还是小很多的。\n\n你可能问了，如果按照这个表有 10000 行来计算的话，C=10000，要是随机到比较大的 Y 值，那扫描行数也跟 20000 差不多了，接近 order by rand() 的扫描行数\n\n取 Y1、Y2 和 Y3 里面最大的一个数，记为 M，最小的一个数记为 N，然后执行下面这条 SQL 语句:\n\n```\nmysql> select * from t limit N, M-N+1;\n```\n\n再加上取整个表总行数的 C 行，这个方案的扫描行数总共只需要 C+M+1 行。\n\n当然也可以先取回 id 值，在应用中确定了三个 id 值以后，再执行三次 where id=X 的语 句也是可以的。\n\n## fun(index)\n\n对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能.\n\n### 一、条件字段函数操作\n\n```\nCREATE TABLE `tradelog` (\n`id` int(11) NOT NULL,\n`tradeid` varchar(32) DEFAULT NULL, \n`operator` int(11) DEFAULT NULL, \n`t_modified` datetime DEFAULT NULL,\nPRIMARY KEY (`id`),\nKEY `tradeid` (`tradeid`),\nKEY `t_modified` (`t_modified`)\nENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\n查询7月份的数据\n\n```\nselect count(*) from tradelog where month(t_modified)=7;\n```\n\n下面是这个 t_modified 索引的示意图。方框上面的数字就是 month() 函数对应的值。\n\n![3P8fGa](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/3P8fGa.png)\n\n如果你的 SQL 语句条件用的是 where t_modified='2018-7-1’的话，引擎就会按照上面 绿色箭头的路线，快速定位到 t_modified='2018-7-1’需要的结果。\n\n实际上，B+ 树提供的这个快速定位能力，来源于同一层兄弟节点的有序性。\n\n但是，如果计算 month() 函数的话，你会看到传入 7 的时候，在树的第一层就不知道该怎 么办了。也就是说，对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。\n\n在这个例子里，放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引 t_modified，优化器对比索引大小后发现，索引 t_modified 更小，遍历这个索引比遍历 主键索引来得更快。因此最终还是会选择索引 t_modified。\n\n### 二、隐式类型转换\n\n我们一起看一下这条 SQL 语句:\n\n```\nmysql> select * from tradelog where tradeid=110717\n```\n\n交易编号 tradeid 这个字段上，本来就有索引，但是 explain 的结果却显示，这条语句需 要走全表扫描。你可能也发现了，tradeid 的字段类型是 varchar(32)，而输入的参数却是整型，所以需要做类型转换\n\n对于优化器来说，这个语句相当于:\n\n```\nmysql> select * from tradelog where CAST(tradid AS signed int) = 110717;\n```\n\n### 三、隐式字符编码转换\n\n两个表的字符集不同，一个是 utf8， 一个是 utf8mb4，做表连接查询的时候用不上关联字段的索引。\n\n```\nselect * from trade_detail where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value;\n```\n\nCONVERT() 函数，在这里的意思是把输入的字符串转成 utf8mb4 字符集。\n\n这就再次触发了我们上面说到的原则:对索引字段做函数操作，优化器会放弃走树搜索功能。\n\n## Join\n\n\n\n## kill\n\nkill 并不是马上停止的意思，而是告诉执行线程说，这条语句已经不需要继续 执行了，可以开始“执行停止的逻辑了”。\n\n其实，这跟 Linux 的 kill 命令类似，kill -N pid 并不是让进程直接停止，而 是给进程发一个信号，然后进程处理这个信号，进入终止逻辑。\n\nkill 无效的第一类情况，即:线程没有执行到判断线程状态的逻辑。\n\n另一类情况是，终止逻辑耗时较长。这时候，从 show processlist 结果上看也是 Command=Killed，需要等到终止逻辑完成，语句才算真正完成。\n\n### kill query thread_id\n\n当用户执行 kill query thread_id_B 时，MySQL 里处理 kill 命令的线程做了两 件事:\n\n1. 把 session B 的运行状态改成 THD::KILL_QUERY (将变量 killed 赋值为 THD::KILL_QUERY) ;\n\n2. 给 session B 的执行线程发一个信号。\n\nsession B 处于锁等待状态，如果只是把 session B 的线程 状态设置 THD::KILL_QUERY，线程 B 并不知道这个状态变化，还是会继续等待。发一个 信号的目的，就是让 session B 退出等待，来处理这个THD::KILL_QUERY 状态。\n\n上面的分析中，隐含了这么三层意思:\n\n1. 一个语句执行过程中有多处“埋点”，在这些“埋点”的地方判断线程状态，如果发现线程状态是 THD::KILL_QUERY，才开始进入语句终止逻辑;\n\n2. 如果处于等待状态，必须是一个可以被唤醒的等待，否则根本不会执行到“埋点”处;\n\n3. 语句从开始进入终止逻辑，到终止逻辑完全完成，是有一个过程的。\n\n### kill connection \n\n而当 session E 执行 kill connection C 命令时，是这么做的，\n\n1. 把 C 线程状态设置为 KILL_CONNECTION;\n\n2. 关掉 C 线程的网络连接。因为有这个操作，所以你会看到，这时候 session C 收到了断开连接的提示。\n\n但是客户端退出了，这个线程的状态仍然是在等待中。那这个线程什么时候会退出呢?\n\n答案是，只有等到满足进入 InnoDB 的条件后，session C 的查询语句继续执行，然后才有可能判断到线程状态已经变成了 KILL_QUERY 或者 KILL_CONNECTION，再进入终止逻辑阶段。\n\n> 参考 《MySQL实战45讲》：https://time.geekbang.org/column/intro/100020801","tags":["database","MySQL"],"categories":["database","MySQL"]},{"title":"MySQL索引笔记","url":"/2020/08/01/MySQL索引/","content":"\n# MySQL索引\n\n## 索引常见模型\n\n哈希表、有序数组和搜索树\n\n<!-- more -->\n\n### 哈希表\n\n哈希表是一种以键 - 值(key-value)存储数据的结构，我们只要输入待查找的值即 key， 就可以找到其对应的值即 Value。哈希的思路很简单，把值放在数组里，用一个哈希函数 把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。\n\n不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的 一种方法是，拉出一个链表。所以哈希索引的缺点是做区间查询的速度很慢。\n\n所以，哈希表这种结构适用于只有等值查询的场景\n\n###有序数组\n\n有序数组在等值查询和范围查询场景中的性能就都非常优秀\n\n查询时使用二分法来查询，这个时间复杂度是 O(log(N))。在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高\n\n所以，有序数组索引只适用于静态存储引擎，比如你要保存的是 2017 年某个城市的所有人口信息，这类不会再修改的数据。\n\n### 二叉树\n\n二叉搜索树的特点是:每个节点的左儿子小于父节点，父节点又小于右儿子。这样如果你要查，按照搜索顺序这个时间复杂度是 O(log(N))。\n\n当然为了维持 O(log(N)) 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 O(log(N))。\n\n树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。\n\n你可以想象一下一棵 100 万节点的平衡二叉树，树高 20。一次查询可能需要访问 20 个数 据块。在机械硬盘时代，从磁盘随机读一个数据块需要 10 ms 左右的寻址时间。也就是说，对于一个 100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要 20 个 10 ms 的时间，这个查询可真够慢的。\n\n为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。\n\n以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一 个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。\n\nN 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。\n\n## InnoDB 的索引模型\n\n在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。\n\n每一个索引在 InnoDB 里面对应一棵 B+ 树。\n\n假设，我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引。 这个表的建表语句是:\n\n```\nmysql> create table T( \nid int primary key,\nk int not null,\nname varchar(16),\nindex (k)\n)engine=InnoDB;\n```\n\n表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树 的示例示意图如下。\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/R0Vc7R.png\" alt=\"R0Vc7R\" style=\"zoom:50%;\" />\n\n从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。\n\n主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引 (clustered index)。\n\n非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引 (secondary index)。\n\n根据上面的索引结构说明，我们来讨论一个问题:基于主键索引和普通索引的查询有什么 区别?\n\n如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树;\n\n如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引 树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。\n\n也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。\n\n### 覆盖索引\n\n如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行?\n\n现在，我们一起来看看这条 SQL 查询语句的执行流程:\n\n1. 在 k 索引树上找到 k=3 的记录，取得 ID = 300;\n2.  2. 再到 ID 索引树查到 ID=300 对应的 R3;\n\n3. 在 k 索引树取下一个值 k=5，取得 ID=500;\n\n4. 再回到 ID 索引树查到 ID=500 对应的 R4;\n\n5. 在 k 索引树取下一个值 k=6，不满足条件，循环结束。\n\n在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读 了 k 索引树的 3 条记录(步骤 1、3 和 5)，回表了两次(步骤 2 和 4)。\n\n如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的 值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是 说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。\n\n由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。\n\n\n\n基于上面覆盖索引的说明，我们来讨论一个问题:在一个市民信息表上，是否有必要将身份证号和名字建立联合索引?\n\n假设这个市民表的定义是这样的:\n\n```\nCREATE TABLE `tuser` (\n`id` int(11) NOT NULL,\n`id_card` varchar(32) DEFAULT NULL,\n`name` varchar(32) DEFAULT NULL, \n`age` int(11) DEFAULT NULL, \n`ismale` tinyint(1) DEFAULT NULL, \nPRIMARY KEY (`id`),\nKEY `id_card` (`id_card`),\nKEY `name_age` (`name`,`age`) \n) ENGINE=InnoDB\n```\n\n我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个(身份证号、姓名)的联合索引，是不是浪费空间?\n\n如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。\n\n当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权 衡考虑了\n\n### 最左前缀原则\n\n看到这里你一定有一个疑问，如果为每一种查询都设计一个索引，索引是不是太多了。如果我现在要按照市民的身份证号去查他的家庭地址呢?虽然这个查询需求在业务中出现的概率不高，但总不能让它走全表扫描吧?反过来说，单独为一个不频繁的请求创建一个(身份证号，地址)的索引又感觉有点浪费。应该怎么做呢?\n\n这里，我先和你说结论吧。B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。\n\n为了直观地说明这个概念，我们用(name，age)这个联合索引来分析。\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/iS3LU2.png\" alt=\"iS3LU2\" style=\"zoom:50%;\" />\n\n可以看到，索引项是按照索引定义里面出现的字段顺序排序的。\n\n当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历 得到所有需要的结果。\n\n如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是\"where name like ‘张 %’\"。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然 后向后遍历，直到不满足条件为止。\n\n可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这 个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。\n\n基于上面对最左前缀索引的说明，我们来讨论一个问题:在建立联合索引的时候，如何安排索引内的字段顺序。\n\n这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。\n\n所以现在你知道了，这段开头的问题里，我们要为高频请求创建 (身份证号，姓名)这个 联合索引，并用这个索引支持“根据身份证号查询地址”的需求。\n\n那么，如果既有联合查询，又有基于 a、b 各自的查询呢?查询条件里面只有 b 的语句， 是无法使用 (a,b) 这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要 同时维护 (a,b)、(b) 这两个索引。\n\n这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个(name,age) 的联合索引和一个 (age) 的单字段 索引。\n\n### 索引下推\n\n上一段我们说到满足最左前缀原则的时候，最左前缀可以用于在索引中定位记录。这时，你可能要问，那些不符合最左前缀的部分，会怎么样呢?\n\n我们还是以市民表的联合索引(name, age)为例。如果现在有一个需求:检索出表 中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的:\n\n```\nmysql> select * from tuser where name like '张 %' and age=10 and ismale=1;\n```\n\n你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第 一个满足条件的记录 ID3。当然，这还不错，总比全表扫描要好。\n\n在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字 段值。\n\n而 MySQL 5.6 引入的索引下推优化(index condition pushdown)， 可以在索引遍历过 程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/15OSYr.png\" alt=\"15OSYr\" style=\"zoom:50%;\" />\n\n### 例子\n\n```\nCREATE TABLE `geek` (\n`a` int(11) NOT NULL, \n`b` int(11) NOT NULL, \n`c` int(11) NOT NULL,\n`d` int(11) NOT NULL,\nPRIMARY KEY (`a`,`b`),\nKEY `c` (`c`),\nKEY `ca` (`c`,`a`), \nKEY `cb` (`c`,`b`) \n) ENGINE=InnoDB;\n```\n\n有查询条件\n\n```\nselect * from geek where c=N order by a limit 1;\nselect * from geek where c=N order by b limit 1;\n```\n\n所以建立了 ‘ca’, 'cb' 两个索引\n\n结论： ca 没有必要， cb 有必要\n\n原因： ca索引，通过索引对数据进行筛选，回表的时候，a本身就是主键索引，所以可以保证有序，所以ca 的索引和 c 的索引是一模一样的 ; cb索引，b上并没有索引，ab索引也无法满足最左匹配原则，可以保留加快排序速度。\n\n### 索引维护\n\nB+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例， 如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。\n\n而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。\n\n除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。\n\n自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。\n\n除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢?\n\n由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级 索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整 型(bigint)则是 8 个字节。\n\n显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。\n\n## 唯一索引\n\n假设你在维护一个市民系统，每个人都有一个唯一的身份证号，而且业务代码已经保证了 不会写入两个重复的身份证号。如果市民系统需要按照身份证号查姓名，就会执行类似这 样的 SQL 语句:\n\n```\nselect name from CUser where id_card = 'xxxxxxxyyyyyyzzzzz';\n```\n\n所以，你一定会考虑在 id_card 字段上建索引。从性能的角度考虑，你选择唯一索引还是普通索引呢?选择的依据是什么呢?\n\n### 查询过程\n\n假设，执行查询的语句是 select id from T where k=5。这个查询语句在索引树上查找的 过程，先是通过 B+ 树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据 页，然后可以认为数据页内部通过二分法来定位记录。\n\n* 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录， 直到碰到第一个不满足 k=5 条件的记录。\n\n* 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。\n\n那么，这个不同带来的性能差距会有多少呢?答案是，微乎其微。\n\n你知道的，InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。\n\n因为引擎是按页读写的，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。\n\n当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。\n\n但是，我们之前计算过，对于整型字段，一个数据页可以放近千个 key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的 CPU 来说可以忽略不计。\n\n### 更新过程\n\n当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。\n\n对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。\n\n因此，唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。\n\n如果在更新时，这个记录要更新的目标页不在内存中。这时，InnoDB 的处理流程如下:\n\n* 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束;\n\n* 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。\n\n## 选错索引\n\n往表 t 中插入 10 万行记录，取值按整数递增，即:(1,1,1)，(2,2,2)，(3,3,3) 直到 (100000,100000,100000)。\n\n```\nCREATE TABLE `t` (\n`id` int(11) NOT NULL,\n`a` int(11) DEFAULT NULL, \n`b` int(11) DEFAULT NULL, \nPRIMARY KEY (`id`),\nKEY `a` (`a`),\nKEY `b` (`b`) \n)ENGINE=InnoDB;\n```\n\n```\ndelimiter ;;\ncreate procedure idata() \nbegin\n\tdeclare i int; \n\tset i=1; \n\twhile(i<=100000)do\n\t\tinsert into t values(i, i, i);\n\t\tset i=i+1; \n\t\tend while;\nend;; \ndelimiter ; \ncall idata();\n```\n\n```\nexplain select * from t where a between 10000 and 20000;\n```\n\n得到的结果是我们所需要的，扫描了10001行，使用了 a 作为索引\n\n![y2tImO](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/y2tImO.png)\n\n发现做了全表扫描，也就是10万行。如果使用 `select * from t force index(a) where a between 10000 and 20000;`来使用强制使用 a 索引，只用扫描 1万行。\n\n这是因为，在事务A还没有提交的时候，事务B删除了10万行的数据不能删除，数据的每一行都有两个版本，这样，索引 a 上就有两份。\n\n优化器得到索引的基数时，预计扫描的行数比原来多了很多，比如原来扫描1万行，现在扫描3万行。尽管还是比10万行的全表扫描要少，但是需要把索引拿出来后回表的代价也算进去。所以优化器选择全表扫描。\n\n解决方法\n\n* force index(a) 来强制使用\n* analyze table t 命令，可以用来重新统计索引信息，让预估重新回到 1 万行\n\n如果有这样的查询语句\n\n```\nselect * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1;\n```\n\n如果使用索引 a 进行查询，那么就是扫描索引 a 的前 1000 个值，然后取到对应的 id，再 到主键索引上去查出每一行，然后根据字段 b 来过滤。显然这样需要扫描 1000 行。\n\n如果使用索引 b 进行查询，那么就是扫描索引 b 的最后 50001 个值，与上面的执行过程 相同，也是需要回到主键索引上取值再判断，所以需要扫描 50001 行。\n\n所以你一定会想，如果使用索引 a 的话，执行速度明显会快很多。结果数据库还是选择了 b 索引。\n\n优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序(b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历)，所以即使扫描行数多，也判定为代价更小。\n\n但是实际上选择 a 索引的代价远比 b 要小。我们如果要纠正选择错误。可以\n\n* force index(a) 来强制使用。但是不够优雅\n* 改成 order by b,a。让数据库认为 a 也需要排序。所以会使用 a 索引\n* 删除 b 索引\n\n## 字符索引\n\n```\nalter table SUser add index index1(email);完整索引\n```\n\n```\nalter table SUser add index index2(email(6));前缀索引\n```\n\n为了解决前缀索引可能导致增加很多的扫描次数，我们依次选取不同长度的前缀来看这个值，比如我们要看一下 4~7 个字节的前缀索引， 可以用这个语句\n\n```\nselect \ncount(distinct left(email,4))as L4, \ncount(distinct left(email,5))as L5,\ncount(distinct left(email,6))as L6,\ncount(distinct left(email,7))as L7,\nfrom SUser; \n```\n\n选择合适的长度来创建前缀索引。\n\n还有像身份证这种前缀不够区分度的，可以来倒序存储。或者加一个字段专门来存校验码。\n\n总结：\n\n1. 直接创建完整索引，这样可能比较占用空间;\n\n2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引;\n\n3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题;\n\n4. 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。\n\n在实际应用中，你要根据业务字段的特点选择使用哪种方式。\n\n> 参考 《MySQL实战45讲》：https://time.geekbang.org/column/intro/100020801\n","tags":["database","MySQL"],"categories":["database","MySQL"]},{"title":"MySQL架构实现细节","url":"/2020/08/01/MySQL底层/","content":"\n# MySQL 底层\n\n## MySQL 架构图\n\n大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。\n\nServer 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数(如日期、时间、数学和加密函数等)，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n\n而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、 MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。\n\n<!-- more -->\n\n![8XS0r9](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/8XS0r9.png)\n\n### 连接器\n\n连接器负责跟客户端建立连接、获取权限、维持和管理连接。\n\n如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。\n\n\n\n客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。\n\n如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒: Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。\n\n数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。\n\n建立连接的过程通常是比较复杂的，建议在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。\n\n但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉 (OOM)，从现象看就是 MySQL 异常重启了。\n\n怎么解决这个问题呢?你可以考虑以下两种方案。\n\n1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后， 断开连接，之后要查询再重连。\n\n2. 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。\n\n### 查询缓存\n\n连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步:查询缓存。\n\nMySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。\n\n如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。\n\n***但是大多数情况下我会建议你不要使用查询缓存，为什么呢?因为查询缓存往往弊大于利。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空***。\n\n可以用 `SHOW VARIABLES LIKE '%query_cache%';`来查看 type 是否开启查询缓存\n\n需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。\n\n### 分析器\n\n分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句， MySQL 需要识别出里面的字符串分别是什么，代表什么。\n\nMySQL 从你输入的\"select\"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。\n\n如果你输入一条 `select * from T where T.name=1;` 如果表T或者字段name在数据库没有的话，那么在分析器层应该返回`Unknown column ‘k’ in ‘where clause`\n\n### 优化器\n\n优化器是在表里面有多个索引的时候，决定使用哪个索引;或者在一个语句有多表关联 (join)的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行 两个表的 join:\n\n```\nmysql> select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20;\n```\n\n既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。\n\n也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。\n\n这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。\n\n### 执行器\n\n开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。\n\n> 为什么要在执行器才分析权限，主要是查询的语句有时候很麻烦，join表，或者有个触发器，很多时候在执行时才能确定\n\n比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的:\n\n1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中;\n\n2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。\n3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。\n\n### 日志模块\n\n#### redo log\n\n在 MySQL 里如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。\n\n当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/s0lyWr.png\" alt=\"s0lyWr\" style=\"zoom: 25%;\" />\n\nwrite pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。\n\nwrite pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。\n\n有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。\n\n#### binlog\n\n前面我们讲过，MySQL 整体来看，其实就有两块:一块是 Server 层，它主要做的是 MySQL 功能层面的事情;还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog(归档日志)。\n\n我想你肯定会问，为什么会有两份日志呢?\n\n因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司 以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。\n\n这两种日志有以下三点不同。\n\n1. redo log 是 InnoDB 引擎特有的;binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。\n\n2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”;binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。\n\n3. redo log 是循环写的，空间固定会用完;binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。\n\nbinlog 有三种格式，分别是 statement、row、mixed。statement 格式的话是记sql语句， row格式会记录行的内容，记两条，更新前和更新后都有，mixed是mysql根据实际情况优化，两种都包含。\n\n有了对这两个日志的概念性理解，我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。\n\n1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器;否则，需要先从磁盘读入内存，然后再返回。\n\n2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。\n\n3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。\n\n4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。\n\n5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交(commit)状态，更新完成。\n\n这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的， 深色框表示是在执行器中执行的。\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/95t4P5.png\" alt=\"95t4P5\" style=\"zoom:50%;\" />\n\nredo log 的写入拆成了两个步骤: prepare 和 commit，这就是\"两阶段提交\"。\n\n## 机制\n\n### crash safe\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/95t4P5.png\" alt=\"95t4P5\" style=\"zoom:50%;\" />\n\n时刻A属于 redolog prepare到写binlog阶段，时刻B属于写 bingo 到 commit阶段。\n\n如果在图中时刻 A 的地方，也就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃(crash)，由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。到这里，大家都可以理解。\n\n大家出现问题的地方，主要集中在时刻 B，也就是 binlog 写完，redo log 还没 commit 前发生 crash，那崩溃恢复的时候 MySQL 会怎么处理?\n\n我们先来看一下崩溃恢复时的判断规则。\n\n1. 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交;\n\n2. 如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整:\n\n   * 如果是，则提交事务; \n\n   * 否则，回滚事务。\n\n**如何判断 binlog是否完整呢？**\n\n* statement 格式的 binlog，最后会有 COMMIT; row 格式的 binlog，最后会有一个 XID event。\n\n**redo log 和 binlog 是怎么关联起来的？**\n\n* 它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log:\n  * 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交;\n  * 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。\n\n### change buffer\n\n当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。\n\n需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。\n\n将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭 (shutdown)的过程中，也会执行 merge 操作。\n\n显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，**提高内存利用率**。\n\n>如果不使用change buffer ，一次性写入很多不在内存的数据的话，那么内存的利用率会急剧下降\n\n我们要在表上执行这个插入语句:\n\n```\nmysql> insert into t(id,k) values(id1,k1),(id2,k2);\n```\n\n这里，我们假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如图 2 所示是带 change buffer 的更新状态图。\n\n![Q55UKs](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/Q55UKs.png)\n\n分析这条更新语句，你会发现它涉及了四个部分:内存、redo log(ib_log_fileX)、 数据表空间(t.ibd)、系统表空间(ibdata1)。 这条更新语句做了如下的操作(按照图中的数字顺序):\n\n1. Page 1 在内存中，直接更新内存;\n\n2. Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息\n\n3. 将上述两个动作记入 redo log 中(图中 3 和 4)。\n\n做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘(两次操作合在一起写了一次磁盘)，而且还是顺序写的。\n\n同时，图中的两个虚线箭头，是后台操作，不影响更新的响应时间。\n\n那在这之后的读请求，要怎么处理呢?比如，我们现在要执行 select * from t where k in (k1, k2)。这里，我画了这两个读请求 的流程图。\n\n如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间(ibdata1)和 redo log(ib_log_fileX)无关了。所以，我在图中就没画出 这两部分。\n\n![0SjiAU](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/0SjiAU.png)\n\n1. 读 Page 1 的时候，直接从内存返回。有几位同学在前面文章的评论中问到，WAL 之后如果读数据，是不是一定要读盘，是不是一定要从 redo log 里面把数据更新以后才可以返回?其实是不用的。你可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据， 但是这里直接从内存返回结果，结果是正确的。\n\n2. 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。\n\n可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存。\n\n所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的 IO 消耗(转成顺序写)，而 change buffer 主要节省的则是随机读磁盘 的 IO 消耗。\n\n#### 使用场景\n\n因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。\n\n反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以， 对于这种业务模式来说，change buffer 反而起到了副作用。\n\n#### 丢失问题\n\nchange buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢?change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢?\n\n答案是不会丢失。虽然是只更新内存，但是在事务提交的时候，我们把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。\n\n### binlog写入\n\nbinlog 的写入逻辑比较简单:事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。\n\n一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。\n\n系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。\n\n事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。状态如图。\n\n![COIPn7](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/COIPn7.png)\n\n可以看到，每个线程有自己 binlog cache，但是共用同一份 binlog 文件。\n\n图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。\n\n图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。\n\nwrite 和 fsync 的时机，是由参数 sync_binlog 控制的:\n\n1. sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync;\n\n2. sync_binlog=1 的时候，表示每次提交事务都会执行 fsync;\n\n3. sync_binlog=N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才fsync。\n\n因此，在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0， 比较常见的是将其设置为 100~1000 中的某个数值。对应的风险是:如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。\n\n#### redo log写入\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/E1Tf41.png\" alt=\"E1Tf41\" style=\"zoom:50%;\" />\n\n为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值:\n\n1. 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ; \n2. 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘;\n\n3. 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。\n\nInnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。\n\n实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。\n\n1. 一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。\n\n2. 另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么 按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。\n\n这里需要说明的是，我们介绍两阶段提交的时候说过，时序上 redo log 先 prepare， 再写 binlog，最后再把 redo log commit。\n\n如果把 innodb_flush_log_at_trx_commit 设置成 1，那么 redo log 在 prepare 阶段就 要持久化一次，因为有一个崩溃恢复逻辑是要依赖于 prepare 的 redo log，再加上 binlog 来恢复的。\n\n每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB 就认为 redo log 在 commit 的时候就不需要 fsync 了，只会 write 到文件系统的 page cache 中就够了。\n\n通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log(prepare 阶段)，一次是 binlog。\n\n#### 组提交 group commit\n\n日志逻辑序列号(log sequence number，LSN)的概念。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。\n\nLSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log。\n\n在并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好。\n\n为了让一次 fsync 带的组员更多，MySQL 有一个很有趣的优化\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/thyqre.png\" alt=\"thyqre\" style=\"zoom:50%;\" />\n\n这么一来，binlog 也可以组提交了。在执行图 5 中第 4 步把 binlog fsync 到磁盘时，如果有多个事务的 binlog 已经写完了，也是一起持久化的，这样也可以减少 IOPS 的消耗。\n\n#### binlog 格式\n\n假设我们输入一个sql语句\n\n```\nmysql> delete from t where a>=4 and t_modified<='2018-11-10' limit 1;\n```\n\n##### statement\n\n当 binlog_format=statement 时，binlog 里面记录的就是 SQL 语句的原文。\n\n运行 `mysql> show binlog events in 'master.000001' `可以看到详情\n\n但是运行这条 delete 命令产生了一个 warning，原因是当前 binlog 设置的是 statement 格式，并且语句中有 limit，所以这个命令可能是 unsafe 的。\n\n为什么这么说呢?这是因为 delete 带 limit，很可能会出现主备数据不一致的情况。比如 上面这个例子:\n\n1. 如果 delete 语句使用的是索引 a，那么会根据索引 a 找到第一个满足条件的行，也就 是说删除的是 a=4 这一行;\n\n2. 但如果使用的是索引 t_modified，那么删除的就是 t_modified='2018-11-09’也就 是 a=5 这一行。\n\n由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情 况:在主库执行这条 SQL 语句的时候，用的是索引 a;而在备库执行这条 SQL 语句的时 候，却使用了索引 t_modified。因此，MySQL 认为这样写是有风险的。\n\n##### row\n\n若是 binlog_format=row 时，binlog 里面记录的就是 SQL 的记录。\n\n如果执行的是 delete 语句，row 格式的 binlog 也会把被删掉的行的整行信息保存起来。\n\n如果执行的是 update 语句的话，binlog 里面会记录修改前整行的数据和修改后的整行数据。\n\n所以当 binlog_format 使用 row 格式的时候，binlog 里面记录了真实删除行的主键 id，这样 binlog 传到备库去的时候，就肯定会删除 id=4 的行，不会有主备删除不同行的问题。\n\n##### mixed\n\n还有一种是 mixed 的格式，为什么会存在statement 和 row 两种格式都存在的情况呢？\n\n因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。\n\n但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如 果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占 用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。\n\n所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的 意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就 用 row 格式，否则就用 statement 格式。\n\n\n\n当然我要说的是，现在越来越多的场景要求把 MySQL 的 binlog 格式设置成 row。这么做的理由有很多，直接看出来的好处: 恢复数据。\n\n因为改动前和改动后的数据都保存着，所以在恢复数据的时候很快也很准确。\n\n### 全表扫描\n\n#### server层\n\nInnoDB 的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表 t 的主键索引。这条查询语句由于没有其他的判断条件，所以查到的每一行都可以直接放到结果集里面，然后返回给客户端。\n\n那么，这个“结果集”存在哪里呢?\n\n实际上，服务端并不需要保存一个完整的结果集。取数据和发数据的流程是这样的:\n\n1. 获取一行，写到 net_buffer 中。这块内存的大小是由参数 net_buffer_length 定义 的，默认是 16k。\n\n2. 重复获取行，直到 net_buffer 写满，调用网络接口发出去。\n\n3. 如果发送成功，就清空 net_buffer，然后继续取下一行，并写入 net_buffer。\n\n4. 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈(socketsend buffer)写满了，进入等待。直到网络栈重新可写，再继续发送。\n\n也就是说，MySQL 是“边读边发的”，这个概念很重要。这就意味着，如果客户端接收得慢，会导致 MySQL 服务端由于结果发不出去，这个事务的执行时间变长。\n\n如果客户端使用–quick 参数，会使用 mysql_use_result 方法。这个方法是读一行处理一行。你可以想象一下，假设有一个业务的逻辑比较复杂，每读一行数据以后要处理的逻辑如果很慢，就会导致客户端要过很久才会去取下一行数据。\n\n因此，对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，我都建议你使用 mysql_store_result 这个接口，直接把查询结果保存到本地内存。\n\n#### InnoDB层\n\n内存的数据页是在 Buffer Pool (BP) 中管理的，在 WAL 里 Buffer Pool 起到了加速更新 的作用。而实际上，Buffer Pool 还有一个更重要的作用，就是加速查询。\n\n而 Buffer Pool 对查询的加速效果，依赖于一个重要的指标，即:内存命中率。\n\n你可以在 show engine innodb status 结果中，查看一个系统当前的 BP 命中率。一般情况下，一个稳定服务的线上系统，要保证响应时间符合要求的话，内存命中率要在 99% 以上。\n\n执行 show engine innodb status ，可以看到“Buffer pool hit rate”字样，显示的就是当前的命中率。InnoDB Buffer Pool 的大小是由参数 innodb_buffer_pool_size 确定的，一般建议设置 成可用物理内存的 60%~80%。\n\nInnoDB 管理 Buffer Pool 的 LRU 算法，是用链表来实现的。\n\n1. 链表头部是 P1，表示 P1 是最近刚刚被访问过的数据页;假设内存里只能放下这么多数据页;\n\n2. 这时候有一个读请求访问 P3，P3 被移到最前面;\n\n3. 这次访问的数据页是不存在于链表中的，所以需要在 Buffer Pool 中新申请一个数据页 Px，加到链表头部。但是由于内存已经满了，不能申请新的内存。于是，会清空链表末尾 Pm 这个数据页的内存，存入 Px 的内容，然后放到链表头部。\n\n4. 从效果上看，就是最久没有被访问的数据页 Pm，被淘汰了。\n\n这个算法乍一看上去没什么问题，但是如果考虑到要做一个全表扫描，会不会有问题呢?\n\n假设按照这个算法，我们要扫描一个 200G 的表，而这个表是一个历史数据表，平时没有业务访问它。那么，按照这个算法扫描的话，就会把当前的 Buffer Pool 里的数据全部淘汰掉，存入扫描过程中访问到的数据页的内容。也就是说 Buffer Pool 里面主要放的是这个历史数据表的数据。对于一个正在做业务服务的库，这可不妙。你会看到，Buffer Pool 的内存命中率急剧下降，磁盘压力增加，SQL 语句响应变慢。\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/2g2vCk.png\" alt=\"2g2vCk\" style=\"zoom:50%;\" />\n\n在 InnoDB 实现上，按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域。图 中 LRU_old 指向的就是 old 区域的第一个位置，是整个链表的 5/8 处。也就是说，靠近链表头部的 5/8 是 young 区域，靠近链表尾部的 3/8 是 old 区域。\n\n改进后的 LRU 算法执行流程变成了下面这样。\n\n1. 图中状态 1，要访问数据页 P3，由于 P3 在 young 区域，因此和优化前的 LRU 算法 一样，将其移到链表头部，变成状态 2。\n\n2. 之后要访问一个新的不存在于当前链表的数据页，这时候依然是淘汰掉数据页 Pm，但是新插入的数据页 Px，是放在 LRU_old 处。\n\n3. 处于 old 区域的数据页，每次被访问的时候都要做下面这个判断: 若这个数据页在 LRU 链表中存在的时间超过了 1 秒，就把它移动到链表头部;\n\n如果这个数据页在 LRU 链表中存在的时间短于 1 秒，位置保持不变。1 秒这个时间，是由参数 innodb_old_blocks_time 控制的。其默认值是 1000，单位毫秒。这个策略，就是为了处理类似全表扫描的操作量身定制的。还是以刚刚的扫描 200G 的历 史数据表为例，我们看看改进后的 LRU 算法的操作逻辑:\n\n1. 扫描过程中，需要新插入的数据页，都被放到 old 区域 ;\n\n2. 一个数据页里面有多条记录，这个数据页会被多次访问到，但由于是顺序扫描，这个数据页第一次被访问和最后一次被访问的时间间隔不会超过 1 秒，因此还是会被保留在old 区域;\n\n3. 再继续扫描后续的数据，之前的这个数据页之后也不会再被访问到，于是始终没有机会移到链表头部(也就是 young 区域)，很快就会被淘汰出去。\n\n可以看到，这个策略最大的收益，就是在扫描这个大表的过程中，虽然也用到了 Buffer Pool，但是对 young 区域完全没有影响，从而保证了 Buffer Pool 响应正常业务的查询命中率。\n\n> 参考 《MySQL实战45讲》：https://time.geekbang.org/column/intro/100020801","tags":["database","MySQL"],"categories":["database","MySQL"]},{"title":"深入了解Elasticsearch","url":"/2020/06/15/ElasticSearch深入/","content":"\n# 深入了解Elasticsearch\n\n## 搜索相关性算分\n\n### 相关性 Relevance\n\n* 搜索的相关性算分,描述了一个文档和查询语句匹配的程度。ES会对每个匹配查询条件的结果进行算分_score\n* 打分的本质是排序,需要把最符合用户需求的文档排在前面。ES 5之前,默认的相关性算分采用TF-IDF, 现在采用BM 25\n\n<!-- more -->\n\n### 词频TF\n\n* Term Frequency:检索词在**一篇文档**中出现的频率\n  * 检索词出现的次数除以文档的总字数\n\n* 度量一条查询和结果文档相关性的简单方法:简单将搜索中每一个词的TF进行相加\n  * TF(区块链)+ TF(的) +TF(应用)\n\n* Stop Word\n  * “的” 在文档中出现了很多次，但是对贡献相关度几乎没有用处，不应该考虑他们的TF\n\n### 逆文档频率IDF\n\n* DF:检索词在**所有文档**中出现的频率\n  * “区块链”在相对比较少的文档中出现\n  * “应用”在相对比较多的文档中出现\n  * “Stop Word”在大量的文档中出现\n\n* Inverse Document Frequency :简单说= log(全部文档数/检索词出现过的文档总数)TF-IDF本质上就是将TF求和变成了加权求和\n  * TF(区块链) DF(区块链) + TF(的) DF(的)+ TF(应用)*IDF(应用)\n\n\n\nLucene 中的 TF-IDF 评分的公式\n\n![7Opaig](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/7Opaig.png)\n\n\n\n## 优化算分\n\n### Function Score Query\n\n可以在查询结束后,对每一个匹配的文档进行一系列的重新算分,根据新生成的分数进行排序。\n\n提供了几种默认的计算分值的函数\n\n* Weight :为每一个文档设置一个简单而不被规范化的权重\n* Field Value Factor:使用该数值来修改_score,例如将“热度”和\"点赞数”作为算分的参考因素\n* Random Score:为每一个用户使用一个不同的,随机算分结果\n* 衰减函数: 以某个字段的值为标准,距离某个值越近,得分越高\n* Script Score:自定义脚本完全控制所需逻辑\n\n**根据字段算分**\n\n比如按照热度、投票算分之类\n\n```\n#根据热度、投票等算分\nPOST covid19/_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": {\n        \"multi_match\": {\n          \"query\":    \"City\",\n          \"fields\": [\"message\"]\n        }\n      },\n      \"field_value_factor\": {\n        \"field\": \"infected\"\n      }\n    }\n  }\n}\n```\n\n**加权搜索**\n\n比如电商给某件商品加权，将之放置在搜索结果的上面\n\n```\nGET /covid19*/_search \n{\n  \"indices_boost\": {\n    \"covid19\": 2,\n    \"covid19-20200409\": 1\n  },\n  \"query\": {\n    \"match\": {\n      \"message\": \"City\"\n    }\n  }\n}\n```\n\n**自动补全**\n\n比如搜索引擎的自动补全功能\n\n```\nDELETE articles\nPUT articles\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title_completion\":{\n        \"type\": \"completion\"\n      }\n    }\n  }\n}\nPOST articles/_bulk\n{ \"index\" : { } }\n{ \"title_completion\": \"lucene is very cool\"}\n{ \"index\" : { } }\n{ \"title_completion\": \"Elasticsearch builds on top of lucene\"}\n{ \"index\" : { } }\n{ \"title_completion\": \"Elasticsearch rocks\"}\n{ \"index\" : { } }\n{ \"title_completion\": \"elastic is the company behind ELK stack\"}\n{ \"index\" : { } }\n{ \"title_completion\": \"Elk stack rocks\"}\n{ \"index\" : {} }\nPOST articles/_search?pretty\n{\n  \"size\": 0,\n  \"suggest\": {\n    \"article-suggester\": {\n      \"prefix\": \"elk\",\n      \"completion\": {\n        \"field\": \"title_completion\"\n      }\n    }\n  }\n}\n```\n\n**搜索建议**\n\n```\nDELETE comments\nPUT comments\nPUT comments/_mapping\n{\n  \"properties\": {\n    \"comment_autocomplete\":{\n      \"type\": \"completion\",\n      \"contexts\":[{\n        \"type\":\"category\",\n        \"name\":\"comment_category\"\n      }]\n    }\n  }\n}\n\nPOST comments/_doc\n{\n  \"comment\":\"I love the star war movies\",\n  \"comment_autocomplete\":{\n    \"input\":[\"star wars\"],\n    \"contexts\":{\n      \"comment_category\":\"movies\"\n    }\n  }\n}\n\nPOST comments/_doc\n{\n  \"comment\":\"Where can I find a Starbucks\",\n  \"comment_autocomplete\":{\n    \"input\":[\"starbucks\"],\n    \"contexts\":{\n      \"comment_category\":\"coffee\"\n    }\n  }\n}\n\n\nPOST comments/_search\n{\n  \"suggest\": {\n    \"MY_SUGGESTION\": {\n      \"prefix\": \"sta\",\n      \"completion\":{\n        \"field\":\"comment_autocomplete\",\n        \"contexts\":{\n          \"comment_category\":\"coffee\"\n        }\n      }\n    }\n  }\n}\n```\n\n## 关联关系\n\n* 关系型数据库,一般会考虑Normalize数据;在Elasticsearch,往往考虑Denormalize数据\n  * Denormalize的好处:读的速度变快/无需表连接/无需行锁\n\n* Elasticsearch并不擅长处理关联关系。我们一般采用以下四种方法处理关联\n  * 对象类型\n  * 嵌套对象(Nested Object)\n  * 父子关联关系(Parent/ Child)\n  *  应用端关联\n\n## 分布式问题\n\n### 如何解决脑裂\n\n脑裂 Split-Brain,分布式系统的经典网络问题,当出现网络问题,一个节点和其他节点无法连接\n\n* Node 2和Node 3会重新选举Master、\n* Node 1自己还是作为Master,组成一个集群,同时更新Cluster State\n* 导致2个master,维护不同的cluster state,当网络恢复时,无法选择正确恢复\n\n![yPfv05](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/yPfv05.png)\n\n#### 如何避免脑裂\n\n限定一个选举条件,设置quorum(仲裁),只有在Master eligible节点数大于quorum时,才能进行选举\n\n* Quorum = (master节点总数/2) +1\n* 当3个master eligible时,设置discovery.zen.minimum_master_nodes为2,即可避免脑裂\n\n从7.0开始，无需这个配置\n\n* 移除minimum_master-nodes参数,让Elasticsearch自己选择可以形成仲裁的节点。\n* 典型的主节点选举现在只需要很短的时间就可以完成。集群的伸缩变得更安全、更容易,并且可能造成丢失数据的系统配置选项更少了。\n* 节点更清楚地记录它们的状态,有助于诊断为什么它们不能加入集群或为什么无法选举出主节点\n\n### 文档分布式同步\n\n文档会存储在具体的某个主分片和副本分片上：例如文档1，会存储在PO和RO分片上\n\n文档到分片的映射算法\n\n* 确保文档能均匀分布在所用分片上,充分利用硬件资源,避免部分机器空闲,部分机器繁忙\n\n* 潜在的算法\n  * 随机/ Round Robin,当查询文档1,分片数很多,需要多次查询才可能查到文档1\n  * 维护文档到分片的映射关系,当文档数据量大的时候,维护成高\n  * 实时计算,通过文档1,自动算出,需要去那个分片上获取文档\n\n#### 文档到分片的路由算法\n\n>  shard = hash(_routing) % number_of_primary_shards\n\n1. Hash算法确保文档均匀分散到分片中\n\n2. 默认的_routing值是文档id\n3. 可以自行制定routing数值,例如用相同国家的商品,都分配到指定的shard\n4. 设置Index Settings后, Primary数,不能随意修改的根本原因\n\n#### 更新文档\n\n![akey43](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/akey43.png)\n\n#### 删除一个文档\n\n![3nCzXD](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/3nCzXD.png)\n\n### 分布式搜索\n\nElasticsearch的搜索,会分两阶段进行\n\n* 第一阶段-Query\n\n* 第二阶段-Fetch\n\n#### Query\n\n1. 用户发出搜索请求到ES节点。节点收到请求后,会以Coordinating节点的身份,在6个主副分片中随机选择3个分片,发送查询请求\n\n2. 被选中的分片执行查询,进行排序。然后，每个分片都会返回From + Size个排序后的文档ld和排序值给Coordinating节点\n\n#### Fetch\n\n1. Coordinating Node会将Query阶段,从从每个分片获取的排序后的文档Id列表,重新进行排序。选取From到From + Size个文档的ld\n\n2. 以multi get请求的方式,到相应的分片获取详细的文档数据\n\n#### 问题\n\n* 性能问题\n  * 每个分片上需要查的文档个数=from + size\n  * 最终协调节点需要处理: number_ofshard * (from+size )\n  * 深度分页\n* 相关性算分\n  * 每个分片都基于自己的分片上的数据进行相关度计算。这会导致打分偏离的情况,特别是数据量很少时。相关性算分在分片之间是相互独立。当文档总数很少的情况下,如果主分片大于1,主分片数越多,相关性算分会越不准\n\n#### 解决\n\n* 数据量不大的时候,可以将主分片数设置为1\n* 当数据量足够大时候,只要保证文档均匀分散在各个分片上,结果一般就不会出现偏差\n* 使用DFS Query Then Fetch\n  * 搜索的URL中指定参数\"_search?search-type-dfs-query-then_fetch\n  * 到每个分片把各分片的词频和文档频率进行搜集,然后完整的进行一次相关性算分,耗费更加多的CPU和内存,执行性能低下,一般不建议使用\n\n### 分页\n\n默认情况下，查询按照相关度算分排序，返回前10条记录。容易理解的分页方案\n\n* From:开始位置\n* Size:期望获取文档的总数\n\n#### 深度分页\n\nES天生就是分布式的。查询信息,但是数据分别保存在多个分片,多台机器上, ES天生就需要满足排序的需要(按照相关性算分)。\n\n当一个查询: From =990, Size =10\n\n* 会在每个分片上先都获取1000个文档。然后,通过Coordinating Node聚合所有结果。最后再通过排序选取前1000个文档\n* 页数越深，占用内存越多。为了避免深度分页带来的内存开销。ES有一个设定，默认限定到10000个文档\n  * Index.max result window\n\n#### search after\n\n```\nPOST users/_search\n{\n    \"size\": 1,\n    \"query\": {\n        \"match_all\": {}\n    },\n    \"search_after\":\n        [\n          10,\n          \"ZQ0vYGsBrR8X3IP75QqX\"\n        ],\n    \"sort\": [\n        {\"age\": \"desc\"} ,\n        {\"_id\": \"asc\"}    \n    ]\n}\n```\n\n每次根据上次的结果查询，比如查询 990 - 1000，那么每个分片查10个然后加在一起排序就行\n\n* 避免深度分页的性能问题，可以实时获取下一页文档信息\n  * 不支持指定页数（From)\n  * 只能往下翻\n\n第一步搜索需要指定sort,并且保证值是唯一的(可以通过加入_id保证唯一性),然后使用上一次，最后一个文档的sort值进行查询\n\n### 处理并发读写\n\n两个Web程序同时更新某个文档,如果缺乏有效的并发，会导致更改的数据丢失\n\n悲观并发控制\n\n* 假定有变更冲突的可能。会对资源加锁,防止冲突。例如数据库行锁\n\n乐观并发控制\n\n* 假定冲突是不会发生的,不会阻塞正在尝试的操作。如果数据在读写中被修改,更新将会失败。应用程序决定如何解决冲突,例如重试更新,使用新的数据,或者将错误报告给用户\n\n#### ES采用的是乐观\n\n并发控制ES中的文档是不可变更的。如果你更新一个文档,会将就文档标记为删除,同时增加一个全新的文档。同时文档的version字段加1\n\n* 内部版本控制\n  * If_seq_no + If_primary_term\n\n* 使用外部版本(使用其他数据库作为主要数据存储)\n  * version + version_type=external\n\n## 对象和关联关系\n\n关系型数据库,一般会考虑Normalize数据;在Elasticsearch,往往考虑Denormalize数据\n\nDenormalize的好：读的速度变快/无需表连接/无需行锁\n\nElasticsearch并不擅长处理关联关系。我们一般采用以下四种方法处理关联\n\n* 对象类型\n* 嵌套对象(Nested Object)\n* 父子关联关系(Parent / Child )\n* 应用端关联\n\n**对象类型**\n\n```\nPUT blog/_doc/1\n{\n  \"content\":\"I like Elasticsearch\",\n  \"time\":\"2020-01-01T00:00:00\",\n  \"user\":{\n    \"userid\":1,\n    \"username\":\"Jack\",\n    \"city\":\"wuhan\"\n  }\n}\nPOST blog/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\"match\": {\"content\": \"Elasticsearch\"}},\n        {\"match\": {\"user.username\": \"Jack\"}}\n      ]\n    }\n  }\n}\n```\n\n**Nested嵌套对象**\n\nNested数据类型：允许对象数组中的对象被独立索引.\n\n使用nested和properties关键字,将所有actors索引到多个分隔的文档在内部, Nested文档会被保存在两个Lucene文档中,在查询时做Join处理.\n\n在内部, Nested文档会被保存在两个Lucene\n\n文档中，会在查询时做Join处理\n\n```\nPUT my_movies\n{\n      \"mappings\" : {\n      \"properties\" : {\n        \"actors\" : {\n          \"type\": \"nested\",\n          \"properties\" : {\n            \"first_name\" : {\"type\" : \"keyword\"},\n            \"last_name\" : {\"type\" : \"keyword\"}\n          }},\n        \"title\" : {\n          \"type\" : \"text\",\n          \"fields\" : {\"keyword\":{\"type\":\"keyword\",\"ignore_above\":256}}\n        }\n      }\n    }\n}\n\n\nPOST my_movies/_doc/1\n{\n  \"title\":\"Speed\",\n  \"actors\":[\n    {\n      \"first_name\":\"Keanu\",\n      \"last_name\":\"Reeves\"\n    },\n\n    {\n      \"first_name\":\"Dennis\",\n      \"last_name\":\"Hopper\"\n    }\n\n  ]\n}\n\n# Nested 查询\nPOST my_movies/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\"match\": {\"title\": \"Speed\"}},\n        {\n          \"nested\": {\n            \"path\": \"actors\",\n            \"query\": {\n              \"bool\": {\n                \"must\": [\n                  {\"match\": {\n                    \"actors.first_name\": \"Keanu\"\n                  }},\n\n                  {\"match\": {\n                    \"actors.last_name\": \"Hopper\"\n                  }}\n                ]\n              }\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n**父子类型**\n\n对象和Nested对象的局限性\n\n* 每次更新，需要重新索引整个对象(包括根对象和嵌套对象)\n\nES提供了类似关系型数据库中Join的实现。使用Join数据类型实现,可以通过维护Parent/ Child的关系，从而分离两个对象\n\n* 父文档和子文档是两个独立的文档\n* 更新父文档无需重新索引子文档。子文档被添加,更新或者删除也不会影响到父文档和其他的子文档\n\n定义父子关系的几个步骤\n\n1. 设置索引的Mapping\n2. 索引父文档\n3. 索引子文档\n4. 按需查询文档\n\n```\nDELETE my_blogs\n\n# 设定 Parent/Child Mapping\nPUT my_blogs\n{\n  \"settings\": {\n    \"number_of_shards\": 2\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"blog_comments_relation\": {\n        \"type\": \"join\",\n        \"relations\": {\n          \"blog\": \"comment\"\n        }\n      },\n      \"content\": {\n        \"type\": \"text\"\n      },\n      \"title\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\n\n#索引父文档\nPUT my_blogs/_doc/blog1\n{\n  \"title\":\"Learning Elasticsearch\",\n  \"content\":\"learning ELK @ geektime\",\n  \"blog_comments_relation\":{\n    \"name\":\"blog\"\n  }\n}\n\n#索引父文档\nPUT my_blogs/_doc/blog2\n{\n  \"title\":\"Learning Hadoop\",\n  \"content\":\"learning Hadoop\",\n    \"blog_comments_relation\":{\n    \"name\":\"blog\"\n  }\n}\n\n\n#索引子文档\nPUT my_blogs/_doc/comment1?routing=blog1\n{\n  \"comment\":\"I am learning ELK\",\n  \"username\":\"Jack\",\n  \"blog_comments_relation\":{\n    \"name\":\"comment\",\n    \"parent\":\"blog1\"\n  }\n}\n\n#索引子文档\nPUT my_blogs/_doc/comment2?routing=blog2\n{\n  \"comment\":\"I like Hadoop!!!!!\",\n  \"username\":\"Jack\",\n  \"blog_comments_relation\":{\n    \"name\":\"comment\",\n    \"parent\":\"blog2\"\n  }\n}\n\n#索引子文档\nPUT my_blogs/_doc/comment3?routing=blog2\n{\n  \"comment\":\"Hello Hadoop\",\n  \"username\":\"Bob\",\n  \"blog_comments_relation\":{\n    \"name\":\"comment\",\n    \"parent\":\"blog2\"\n  }\n}\n\n# 查询所有文档\nPOST my_blogs/_search\n{\n\n}\n\n\n#根据父文档ID查看\nGET my_blogs/_doc/blog2\n\n# Parent Id 查询\nPOST my_blogs/_search\n{\n  \"query\": {\n    \"parent_id\": {\n      \"type\": \"comment\",\n      \"id\": \"blog2\"\n    }\n  }\n}\n\n# Has Child 查询,返回父文档\nPOST my_blogs/_search\n{\n  \"query\": {\n    \"has_child\": {\n      \"type\": \"comment\",\n      \"query\" : {\n                \"match\": {\n                    \"username\" : \"Jack\"\n                }\n            }\n    }\n  }\n}\n\n\n# Has Parent 查询，返回相关的子文档\nPOST my_blogs/_search\n{\n  \"query\": {\n    \"has_parent\": {\n      \"parent_type\": \"blog\",\n      \"query\" : {\n                \"match\": {\n                    \"title\" : \"Learning Hadoop\"\n                }\n            }\n    }\n  }\n}\n\n\n\n#通过ID ，访问子文档\nGET my_blogs/_doc/comment3\n#通过ID和routing ，访问子文档\nGET my_blogs/_doc/comment3?routing=blog2\n\n#更新子文档\nPUT my_blogs/_doc/comment3?routing=blog2\n{\n    \"comment\": \"Hello Hadoop??\",\n    \"blog_comments_relation\": {\n      \"name\": \"comment\",\n      \"parent\": \"blog2\"\n    }\n}\n```\n\n相关文档\n\n- https://www.elastic.co/guide/en/elasticsearch/reference/7.1/query-dsl-has-child-query.html\n- https://www.elastic.co/guide/en/elasticsearch/reference/7.1/query-dsl-has-parent-query.html\n- https://www.elastic.co/guide/en/elasticsearch/reference/7.1/query-dsl-parent-id-query.html\n- https://www.elastic.co/guide/en/elasticsearch/reference/7.1/query-dsl-parent-id-query.html\n\n","tags":["database","ElasticSearch"],"categories":["database","ElasticSearch"]},{"title":"ElasticSearch初识","url":"/2020/06/15/ElasticSearch初识/","content":"\n# ElasticSearch初识\n\n## ElasticSearch的历史\n\n* ElasticSearch 是一款基于 Lunece 的开源分布式搜索分析引擎\n  * 查询性能好\n  * 分布式设计，非常方便的水平扩展\n  * 支持多种多种语言的集成\n* 超过 2.5 亿的下载量，有良好的开发者社区，更有商业公司的支持\n\n<!-- more -->\n\n### Elastic Stack 生态\n\n\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/AaudpE.png\" style=\"zoom: 25%;\" />\n\n* 应用场景\n  * 网站搜索 / 垂直搜索 /代码搜索\n  * 日志管理与分析 / 安全指标监控 / 应用性能监控 / WEB抓取舆情分析\n\n\n\n## ElasticSearch 的基本概念\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/fwSAVL.png\" style=\"zoom:50%;\" />\n\n### 文档（Document）\n\n* ElasticSearch 是面向文档的，文档上所有可搜索数据的最小单位\n  * 日志文件中的日志项\n  * 一部电影的具体信息\n  * 一篇 PDF 文档中的具体消息\n* 文档会被序列化成 JSON 格式，保存在 ElasticSearch 中\n  * JSON 对象由字段组成\n  * 每个字段都有对应的字段类型（字符串 / 数值 / 布尔 / 日期 / 二进制）\n* 每个文档都有一个 Unique ID\n  * 可以自己指定ID\n  * 或者通过 ElasticSearch 主动生成\n\n**文档的元数据**\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/7ENYpp.png)\n\nPS:`_type`在7.0之前可以设置多个Types，7.0之后一个索引只能创建一个Type-“_doc”\n\n### 索引（Index）\n\n* Index 索引是文档的容器，是一类文档的结合\n  * Index体现了逻辑概念，每个索引都有自己的 Mapping 定义\n  * Shard 体现了物理概念，索引中的数据分散在 Shard 上\n* 索引的 Mapping 和 Setting\n  * Mapping 定义文档字段的类型\n  * Setting 定义不同的数据分布\n\n### REST API 通信\n\n![WumWf5](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/WumWf5.png)\n\n\n\n### 节点\n\n* 节点上一个 ElasticSearch 的实例，本质上一个 Java 进程\n* 每个节点都有名字，通过配置文件配置，或者启动时候 -E node.name=node1 指定\n* 每个节点在启动后，会分配一个UID，保存在 data 目录下\n\n#### Master-eligible nodes 和 Master Node\n\n* 每个节点启动后，默认就是一个 Master eligible 节点\n* Master-eligible 节点可以参加竞选主流程，成为 Master节点\n* 当第一个节点启动时，它会将自己选举成 Master 节点\n* 每个节点都保存了集群的状态，但只有Master节点才能修改集群的状态消息\n  * 集群状态(Cluster State)，维护了一个集群中的消息（所有节点信息、所有索引信息、分片的路由信息）\n  * 任意的节点都能修改信息会导致数据的不一致\n\n#### Data Node 和 Coordinating Node\n\n* Data Nod 是可以保存数据的节点，负责保存分片数据，在数据扩展上起到关键作用\n* Coordinating Node 负责接收 Client 的请求，将请求分发到合适的节点，最终把结果汇聚到一起，每个节点都默认的起到了Coordinating Node 的职责\n\n#### 其它的节点类型\n\n* Hot & Warm Node\n  * 不同硬件配置的 Data Node，用来实现 Hot & Warm 架构，降低集群的部署成本\n* Machine Learning Node\n  * 负责机器学习来做异常检测之类\n\n**发展过程**\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/8LSTpe.png\" style=\"zoom: 33%;\" />\n\n### 分片（Primary Shard & Replica Shard）\n\n* 主分片，用来解决数据水平扩展的问题。通过主分片，可以将数据分布到集群的所有节点上\n  * 一个分片上一个运行的 Lucene 的实例\n  * 主分片数在索引创建时指定，后面不允许修改，除非 Reindex\n* 副本，用来解决数据高可用的问题，副本分片是主分片的拷贝\n  * 副本分片数，可以动态调整\n  * 增加副本数，可以在一定程度提高服务的可用性（读取的吞吐）\n\n![fta6Ef](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/fta6Ef.jpg)\n\n如图所示，创建一个索引，这个索引可以拆分成多个 `shard`，每个 shard 存储部分数据。多个 shard 的好处是\n\n* **支持横向扩展**，如果你的数据量是 3T，分成 3 个 shard，每个 shard 就是 1T 的数据，若现在数据量增加到 4T，要保持每个 shard 都是1T 的数据，那么重新建一个有 4 个 shard 的索引，将数据导入\n* **提高性能**，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能\n* **高可用性**，每个 shard 的数据都有多个备份，如果某个机器宕机了还有数据副本在其它的机器上\n\nshard 的数据有多个备份，每个 shard 都有一个 `primary shard`，负责写入数据，还有几个 `replica shard`。`primary shard` 写入数据之后，会将数据同步到其他几个 `replica shard` 上去。\n\nes 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点负责维护索引元数据、负责切换 primary shard 和 replica shard 身份等。如果 master 节点宕机了，那么会重新选举一个节点为 master 节点。\n\n如果是非 master节点宕机了，那么会由 master 节点，将宕机节点上的 primary shard 的身份转移到其它机器上的 replica shard。接着修复了宕机机器重启了之后，master 节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据之类的，让集群恢复正常。如果宕机的机器修复了，修复后的节点也不再是 primary shard，而是 replica shard。\n\n上述就是 ElasticSearch 作为分布式搜索引擎最基本的一个架构设计。\n\n\n\n### 倒排索引\n\n#### 正排索引和倒排索引\n\n![f5Mnaz](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/f5Mnaz.png)\n\n* 正排索引：文档ID到文档内容和单词的关联\n* 倒排索引：单词到文档ID到关联\n\n#### 倒排索引的核心组成\n\n包括两个部分\n\n* 单词词典（Term Dictionary），记录所有文档的单词，记录单词到倒排列表到关联关系\n  * 单词词典一般比较大，可以通过 B+ 树或哈希拉链实现，以满足高性能的插入和查询\n* 倒排列表（Posting List），记录单词对应的文档结合，由倒排索引项组成\n  * 倒排索引项（Posting）\n    * 文档ID\n    * 词频 TF - 该单词在文档中出现的次数，用来相关性评分\n    * 位置（Posting） - 单词中文档中的位置，用来语句搜索\n    * 偏移（Offset） - 记录单词开始和结束的位置，实现高亮显示\n\n​\t\t\n\n###Analysis 和 Analyzer\n\nAnalysis 文本分析是把全文本转换一系列单词（term / token）的过程，也叫分词，Analysis 是通过 Analyzer 分词器来实现的。Analyzer是专门处理分词的组件，Analyzer 由三部分组成\n\n* Character Filters - 针对原始文本处理，例如去除html\n* Tokenizer  - 按照规则切分为单词\n* Token Filter - 将切分的单词进行加工，小写，删除 stopwords，增加同义词\n\n![BTKiF8](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/BTKiF8.png)\n\n#### Elasticsearch 的内置分词器\n\n* Standard Analyzer-默认分词器,按i切分,小写处理\n* Simple Analyzer-按照非字母切分(符号被过滤) ,小写处理\n* Stop Analyzer-小写处理,停用词过滤(the, a, is)\n* Whitespace Analyzer-按照空格切分,不转小写\n* Keyword Analyzer-不分词,直接将输入当作输出\n* Patter Analyzer-正则表达式,默认\\W+(非字符分隔)\n* Language-提供了30多种常见语言的分词器\n* Customer Analyzer自定义分词器\n\n#### 中文分词\n\n一句中文，在不同的语境下，有不同的理解\n\n* 这个苹果，不大好吃 / 这个苹果，不大，好吃！\n\n中文分词有很多歧义的语境（组合型歧义、交集型歧义、真歧义）\n\n##### 中文分词的演变\n\n* 查字典 - 最容易想到的分词方法(北京航空大学的梁南元教授提出)\n  * 一个句子从左到右扫描一遍。遇到有的词就标示出来。找到复合词,就找最长的\n  * 不认识的字串就分割成单字词\n\n* 最小词数的分词理论 - 哈工大王晓龙博士把查字典的方法理论化\n  * 一句话应该分成数量最少的词串\n  * 遇到二义性的分割，无能为力（例如:“发展中国家” /“上海大学城书店\"）\n  * 用各种文化规则来解决二义性,都并不成功\n\n* 统计语言模型-1990年前后,清华大学电子工程系郭进博士\n  * 解决了二义性问题,将中文分词的错误率降低了一个数量级。概率问题,动态规划+利用维特比算法快速找到最佳分词\n\n* 基于统计的机器学习算法\n  * 这类目前常用的是算法是HMM、CRF、 SVM、深度学习等算法。比如Hanlp分词工具是基于CRF算法以CRF为例，基本思路是对汉字进行标注训练,不仅考虑了词语出现的频率,还考虑上下文,具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。\n  * 随着深度学习的兴起,也出现了基于神经网络的分词器,有人尝试使用双向LSTM+CRF实 分词器,其本质上是序列标注,据报道其分词器字符准确率可高达97.5%\n\n###### ICU Analyzer\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/fHtrDa.png\" alt=\"fHtrDa\" style=\"zoom:50%;\" />\n\n### 聚合(Aggregation)\n\n* Elasticsearch 除了搜索以外，提供的针对 ES 数据进行统计分析的功能\n  * 实时性高，Hadoop需要 (T+1) 天\n* 通过聚合，我们得到的是一个数据的概览，是分析和总结的数据，而不是单个文档\n* 高性能，只需要一句语句，就可以从 Elasticsearch中得到分析结果，无需自己去实现逻辑\n\n#### 集合的分类\n\n* Bucket Aggregation - 一些列满足特定的文档的集合\n* Metric Aggregation - 一些数学运算，可以对文档字段进行统计分析（min / max / avg / sum / cardinality）\n* Pipeline Aggregation - 对其它的聚合结果进行二次聚合\n* Matrix Aggregation - 支持对多个字段的操作并提供一个结果矩阵\n\n## ElasticSearch 的工作机制\n\n###写入数据\n\n- 客户端选择一个 node 发送请求，这个 node 就是 `coordinating node`（协调节点）。\n- `coordinating node` 对 document 进行**路由**，将请求转发给对应的 node（有 primary shard）。\n- 实际的 node 上的 `primary shard` 处理请求，然后将数据同步到 `replica node`。\n- `coordinating node` 在发现 `primary node` 和所有 `replica node` 都完成之后，就返回响应结果给客户端。\n\n![2ARRY7](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/2ARRY7.jpg)\n\n### 读取数据\n\n- 客户端发送请求到**任意**一个 node，成为 `coordinate node`。\n- `coordinate node` 对 `doc id` 进行哈希路由，将请求转发到对应的 node，此时会使用 `round-robin` **随机轮询算法**，在 `primary shard` 以及其所有 replica 中随机选择一个，让读请求负载均衡。\n- 接收请求的 node 返回 document 给 `coordinate node`。\n- `coordinate node` 返回 document 给客户端。\n\n### 搜索数据\n\n- 客户端发送请求到一个 `coordinate node`。\n- 协调节点将搜索请求转发到**所有**的 shard 对应的 `primary shard` 或 `replica shard`。\n- query phase：每个 shard 将自己的搜索结果（其实就是一些 `doc id`）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。\n- fetch phase：接着由协调节点根据 `doc id` 去各个节点上**拉取实际**的 `document` 数据，最终返回给客户端。\n\n### 数据的生命周期\n\n#### 写入数据\n\n* 在Lucene中，单个倒排索引文件被称为Segment。 Segment是自包含的，不可变更的。多个Segments汇总在一起,称为Lucene的Index，其对应的就是ES中的Shard。\n\n* 当有新文档写入时，会生成新Segment，查询时会同时查询所有Segments，并且对结果汇总。Lucene中有一个文件，用来记录所有Segments信息，叫做Commit Point。\n\n* 删除的文档信息，保存在\".del\"文件中。\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/5bz0Ij.jpg\" alt=\"5bz0Ij\" style=\"zoom:67%;\" />\n\n1. 首先将写入内存 buffer 中，同时写入 translog 日志文件，此时数据不可以被搜索到。\n\n2. 如果 buffer 满了，或者每隔 1 秒钟(可修改)，es 将 buffer 中的数据写入一个**新的** `segment file`，每秒钟会产生一个**新的磁盘文件** `segment file`，这个 `segment file` 中就存储最近 1 秒内 buffer 中写入的数据。\n\n3. 但是因为直接写入磁盘的代价很大，所以此时数据不是直接进入磁盘，而是先进入 `os cache` 内存中。这个过程就是 `refresh`。只要 `buffer` 中的数据被 refresh 操作刷入 `os cache`中，这个数据就可以被搜索到了。\n\n4. 新的数据不断进入 buffer 和 translog，不断将 `buffer` 数据写入一个又一个新的 `segment file` 中去，每次 `refresh` 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 `commit` 操作。\n\n5. commit 操作发生第一步，就是将 buffer 中现有数据 `refresh` 到 `os cache` 中去，清空 buffer。然后，将一个 `commit point` 写入磁盘文件，里面标识着这个 `commit point` 对应的所有 `segment file`，同时强行将 `os cache` 中目前所有的数据都 `fsync` 到磁盘文件中去。最后**清空** 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。这个 commit 操作叫做 `flush`。默认 30 分钟自动执行一次 `flush`，但如果 translog 过大，也会触发 `flush`。\n\n#### 删除数据\n\n如果是删除操作，commit 的时候会生成一个 `.del` 文件，里面将某个 doc 标识为 `deleted` 状态，那么搜索的时候根据 `.del` 文件就知道这个 doc 是否被删除了。\n\n如果是更新操作，就是将原来的 doc 标识为 `deleted` 状态，然后新写入一条数据。\n\nbuffer 每 refresh 一次，就会产生一个 `segment file`，所以默认情况下是 1 秒钟一个 `segment file`，这样下来 `segment file` 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 `segment file` 合并成一个，同时这里会将标识为 `deleted` 的 doc 给**物理删除掉**，然后将新的 `segment file` 写入磁盘，这里会写一个 `commit point`，标识所有新的 `segment file`，然后打开 `segment file` 供搜索使用，同时删除旧的 `segment file`。","tags":["database","ElasticSearch"],"categories":["database","ElasticSearch"]},{"title":"Kafka进阶","url":"/2020/05/17/kafka2/","content":"# Kafka进阶\n\n## Broker的设计\n\nBroker 是Kafka 集群中的节点。负责处理生产者发送过来的消息，消费者消费的请求。以及集群节点的管理等。\n\n<!-- more -->\n\n<img src=\"https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/wE4bYT.png\" alt=\"zooker\" style=\"zoom:50%;\" />\n\nZooKeeper 主要为 Kafka 提供元数据的管理的功能。\n\n从图中我们可以看出，Zookeeper 主要为 Kafka 做了下面这些事情：\n\n1. **Broker 注册** ：在 Zookeeper 上会有一个专门**用来进行 Broker 服务器列表记录**的节点。每个 Broker 在启动时，都会到 Zookeeper 上进行注册，即到/brokers/ids 下创建属于自己的节点。每个 Broker 就会将自己的 IP 地址和端口等信息记录到该节点中去\n2. **Topic 注册** ： 在 Kafka 中，同一个**Topic 的消息会被分成多个分区**并将其分布在多个 Broker 上，**这些分区信息及与 Broker 的对应关系**也都是由 Zookeeper 在维护。比如我创建了一个名字为 mytopic 的主题并且它有两个分区，对应到 zookeeper 中会创建这些文件夹：`/brokers/topics/mytopic/Partitions/0`、`/brokers/topics/mytopic/Partitions/1`\n3. **负载均衡** ：上面也说过了 Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力。 对于同一个 Topic 的不同 Partition，Kafka 会尽力将这些 Partition 分布到不同的 Broker 服务器上。当生产者产生消息后也会尽量投递到不同 Broker 的 Partition 里面。当 Consumer 消费的时候，Zookeeper 可以根据当前的 Partition 数量以及 Consumer 数量来实现动态负载均衡。\n4. **故障转移**：在r `/brokers/topics/[topic]/partitions/[partition]/state` 保存了topic-partition的leader和Isr等信息。**Controller负责broker故障检查&&故障转移（fail/recover）**。\n\n### broker负载均衡\n\n* 分区数量负载：各台broker的partition数量应该均匀\n\npartition Replica分配算法如下：\n\n1. 将所有Broker（假设共n个Broker）和待分配的Partition排序\n2. 将第i个Partition分配到第（i mod n）个Broker上\n3. 将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上\n\n\n\n* 容量大小负载：每台broker的硬盘占用大小应该均匀\n\n在kafka1.1之前，Kafka能够保证各台broker上partition数量均匀，但由于每个partition内的消息数不同，可能存在不同硬盘之间内存占用差异大的情况。在Kafka1.1中增加了副本跨路径迁移功能kafka-reassign-partitions.sh，我们可以结合它和监控系统，实现自动化的负载均衡\n\n### borker故障转移\n\n#### broker宕机\n\n1. Controller在Zookeeper上注册Watch，一旦有Broker宕机，其在Zookeeper对应的znode会自动被删除，Zookeeper会触发 Controller注册的watch，Controller读取最新的Broker信息\n2. Controller确定set_p，该集合包含了宕机的所有Broker上的所有Partition\n3. 对set_p中的每一个Partition，选举出新的leader、Isr，并更新结果，从`/brokers/topics/[topic]/partitions/[partition]/state`读取该Partition当前的ISR\n4. 决定该Partition的新Leader和Isr。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失)\n5. 更新Leader、ISR、leader_epoch、controller_epoch：写入`/brokers/topics/[topic]/partitions/[partition]/state`。直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。\n\n#### Controller宕机\n\n每个 broker 都会在 zookeeper 的临时节点 \"/controller\" 注册 watcher，当 controller 宕机时 \"/controller\" 会消失，触发broker的watch，每个 broker 都尝试创建新的 controller path，只有一个竞选成功并当选为 controller。\n\n## kafka的高吞吐\n\n消息中间件从功能上看就是写入数据、读取数据两大类，优化也可以从这两方面来看。\n\n### 写入\n\n为了优化写入速度 Kafak 采用以下技术：\n\n#### 1. 顺序写入\n\n磁盘大多数都还是机械结构（SSD不在讨论的范围内），如果将消息以随机写的方式存入磁盘，就需要按柱面、磁头、扇区的方式寻址，缓慢的机械运动（相对内存）会消耗大量时间，导致磁盘的写入速度与内存写入速度差好几个数量级。为了规避随机写带来的时间消耗，Kafka 采取了顺序写的方式存储数据，如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/UurWaQ.jpg)\n\n每条消息都被append 到该 partition 中，属于顺序写磁盘，因此效率非常高。 但这种方法有一个缺陷：没有办法删除数据。所以Kafka是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个 Topic 都有一个 offset 用来表示读取到了第几条数据。\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/1SrEJm.jpg)\n\n上图中有两个消费者，Consumer1 有两个 offset 分别对应 Partition0、Partition1（假设每一个 Topic 一个 Partition ）。Consumer2 有一个 offset 对应Partition2 。这个 offset 是由客户端 SDK 保存的，Kafka 的 Broker 完全无视这个东西的存在，一般情况下 SDK 会把它保存到 zookeeper 里面。 如果不删除消息，硬盘肯定会被撑满，所以 Kakfa 提供了两种策略来删除数据。一是基于时间，二是基于 partition 文件大小，具体配置可以参看它的配置文档。 即使是顺序写，过于频繁的大量小 I/O 操作一样会造成磁盘的瓶颈，所以 Kakfa 在此处的处理是把这些消息集合在一起批量发送，这样减少对磁盘 I/O 的过度操作，而不是一次发送单个消息。\n\n#### 2. 内存映射文件\n\n即便是顺序写入硬盘，硬盘的访问速度还是不可能追上内存。所以 Kafka 的数据并不是实时的写入硬盘，它充分利用了现代操作系统分页存储来利用内存提高I/O效率。Memory Mapped Files （后面简称mmap）也被翻译成内存映射文件，在64位操作系统中一般可以表示 20G 的数据文件，它的工作原理是直接利用操作系统的 Page 来实现文件到物理内存的直接映射。完成映射之后对物理内存的操作会被同步到硬盘上（由操作系统在适当的时候）。 通过 mmap 进程像读写硬盘一样读写内存，也不必关心内存的大小，有虚拟内存为我们兜底。使用这种方式可以获取很大的 I/O 提升，因为它省去了用户空间到内核空间复制的开销（调用文件的 read 函数会把数据先放到内核空间的内存中，然后再复制到用户空间的内存中） 但这样也有一个很明显的缺陷——不可靠，写到 mmap 中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用 flush 的时候才把数据真正的写到硬盘。所以 Kafka 提供了一个参数—— producer.type 来控制是不是主动 flush，如果Kafka 写入到 mmap 之后就立即 flush 然后再返回 Producer 叫同步(sync)；如果写入 mmap 之后立即返回，Producer 不调用 flush ，就叫异步(async)。\n\n#### 3. 标准化二进制消息格式\n\n为了避免无效率的字节复制，尤其是在负载比较高的情况下影响是显著的。为了避免这种情况，Kafka 采用由 Producer，Broker 和 Consumer 共享的标准化二进制消息格式，这样数据块就可以在它们之间自由传输，无需转换，降低了字节复制的成本开销。\n\n而在读取速度的优化上 Kafak 采取的主要是零拷贝\n\n### 读取\n\n#### 零拷贝（Zero Copy）的技术：\n\n传统模式下我们从硬盘读取一个文件是这样的\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/acY0zH.jpg)\n\n(1) 操作系统将数据从磁盘读到内核空间的页缓存区\n\n(2) 应用将数据从内核空间读到用户空间的缓存中\n\n(3) 应用将数据写会内核空间的套接字缓存中\n\n(4)操作系统将数据从套接字缓存写到网卡缓存中，以便将数据经网络发出\n\n这样做明显是低效的，这里有四次拷贝，两次系统调用。 针对这种情况 Unix 操作系统提供了一个优化的路径，用于将数据从页缓存区传输到 socket。在 Linux 中，是通过 sendfile 系统调用来完成的。Java提供了访问这个系统调用的方法：FileChannel.transferTo API。这种方式只需要一次拷贝：操作系统将数据直接从页缓存发送到网络上，在这个优化的路径中，只有最后一步将数据拷贝到网卡缓存中是需要的。\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/M331Ew.jpg)\n\n零拷贝是指内核空间和用户空间的交互的拷贝次数为零。这个技术其实非常普遍，Nginx 也是用的这种技术。\n\n## MQ常见问题Kafka解决思路\n\n### 幂等性\n\n既然是消费消息，那肯定要考虑会不会重复消费？能不能避免重复消费？或者重复消费了也别造成系统异常可以吗？\n\n**什么时候Kafka会发生重复消费？**\n\nKafka 实际上有个 offset 的概念，每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，**每隔一段时间**（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次继续从上次消费到的 offset 继续消费。\n\n但是凡事总有意外，就是你有时候重启系统，碰到着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset。重启之后，少数消息会再次消费一次。\n\n![Kafka](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/7vBGuj.jpg)\n\n上述图片即是一个重复消费的例子，消费者在消费玩offset=153后被重启，没有来得及将offset提交给Kafka。所以重启后再一次消费了offset=152和offset=153。\n\n\n\n其实重复消费有时候无法避免，那么我们就需要考虑到在重复消费发生时，保持系统的幂等性。\n\n具体需要结合业务，比如：\n\n* 如果是数据库写入，首先根据主键查询，如果已经有了这一条数据，那么就执行update\n* 如果是redis，那么就不需要考虑，因为set的操作天然具有幂等性\n* 如果更复杂的话，我们在生产者发送数据时，加一个全局唯一的ID，当消费ID的时候，去redis里面查询是否消费过，如果消费过就忽略，保证不处理同样的消息\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/nwZKx4.jpg)\n\n### 可靠性传输\n\n如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？\n\n#### 消费端丢失数据\n\n唯一可能导致消费者弄丢数据的情况，就是当消费到了这个消息，然后消费者那边**自动提交了 offset**，让 Kafka 以为消费者已经消费好了这个消息，但其实消费者才刚准备处理这个消息，还没处理完就挂了，此时这条消息就丢啦。\n\n从上面知道 Kafka 会自动提交 offset，那么只要**关闭自动提交** offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是**可能会有重复消费**，比如消费者刚处理完，还没提交 offset，结果消费者挂了，此时肯定会重复消费一次，需要自己保证幂等性。\n\n#### Kafka丢失数据\n\n这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，于是就少了一些数据。\n\n生产环境中，Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。\n\n所以此时一般是要求起码设置如下 4 个参数：\n\n- 给 topic 设置 `replication.factor` 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。\n- 在 Kafka 服务端设置 `min.insync.replicas` 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。\n- 在 producer 端设置 `acks=all`：这个是要求每条数据，必须是**写入所有 replica 之后，才能认为是写成功了**。\n- 在 producer 端设置 `retries=MAX`（很大很大很大的一个值，无限次重试的意思）：这个是**要求一旦写入失败，就无限重试**，卡在这里了。\n\n我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。\n\n#### 生产者会不会弄丢数据？\n\n如果按照上述的思路设置了 `acks=all`，一定不会丢，要求是，Kafka的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。\n\n### 消息的顺序性\n\n如何保证消息的顺序性？\n\n举一个例子，在mysql `binlog` 同步的系统中：在 mysql 里增删改一条数据，对应出来了增删改 3 条 `binlog` 日志，接着这三条 `binlog` 发送到 MQ 里面，再消费出来依次执行，需要保证是按照顺序来的。不然本来是：增加、修改、删除；如果换了顺序给执行成删除、修改、增加，那么就全错了。本来这个数据同步过来，应该最后这个数据被删除了；结果搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。\n\n#### Kafka生产环境中出错的场景\n\n比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。\n消费者从 partition 中取出来数据的时候，也一定是有顺序的。但是接着，我们在消费者里可能会需要**多个线程来并发处理消息**。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/XdEjT3.jpg)\n\n#### 解决方法\n\n- 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低。\n- 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/wAvtnx.jpg)","tags":["MQ","kafka"],"categories":["MQ","kafka"]},{"title":"Kafka高吞吐","url":"/2020/05/17/kafka3/","content":"# Kafka 高吞吐量的秘诀\n\n消息中间件从功能上看就是写入数据、读取数据两大类，优化也可以从这两方面来看。\n\n为了优化写入速度 Kafak 采用以下技术：\n\n### 1. 顺序写入\n\n磁盘大多数都还是机械结构（SSD不在讨论的范围内），如果将消息以随机写的方式存入磁盘，就需要按柱面、磁头、扇区的方式寻址，缓慢的机械运动（相对内存）会消耗大量时间，导致磁盘的写入速度与内存写入速度差好几个数量级。为了规避随机写带来的时间消耗，Kafka 采取了顺序写的方式存储数据，如下图所示：\n\n<!-- more -->\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/UurWaQ.jpg)\n\n每条消息都被append 到该 partition 中，属于顺序写磁盘，因此效率非常高。 但这种方法有一个缺陷：没有办法删除数据。所以Kafka是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个 Topic 都有一个 offset 用来表示读取到了第几条数据。\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/1SrEJm.jpg)\n\n上图中有两个消费者，Consumer1 有两个 offset 分别对应 Partition0、Partition1（假设每一个 Topic 一个 Partition ）。Consumer2 有一个 offset 对应Partition2 。这个 offset 是由客户端 SDK 保存的，Kafka 的 Broker 完全无视这个东西的存在，一般情况下 SDK 会把它保存到 zookeeper 里面。 如果不删除消息，硬盘肯定会被撑满，所以 Kakfa 提供了两种策略来删除数据。一是基于时间，二是基于 partition 文件大小，具体配置可以参看它的配置文档。 即使是顺序写，过于频繁的大量小 I/O 操作一样会造成磁盘的瓶颈，所以 Kakfa 在此处的处理是把这些消息集合在一起批量发送，这样减少对磁盘 I/O 的过度操作，而不是一次发送单个消息。\n\n\n\n### 2. 内存映射文件\n\n即便是顺序写入硬盘，硬盘的访问速度还是不可能追上内存。所以 Kafka 的数据并不是实时的写入硬盘，它充分利用了现代操作系统分页存储来利用内存提高I/O效率。Memory Mapped Files （后面简称mmap）也被翻译成内存映射文件，在64位操作系统中一般可以表示 20G 的数据文件，它的工作原理是直接利用操作系统的 Page 来实现文件到物理内存的直接映射。完成映射之后对物理内存的操作会被同步到硬盘上（由操作系统在适当的时候）。 通过 mmap 进程像读写硬盘一样读写内存，也不必关心内存的大小，有虚拟内存为我们兜底。使用这种方式可以获取很大的 I/O 提升，因为它省去了用户空间到内核空间复制的开销（调用文件的 read 函数会把数据先放到内核空间的内存中，然后再复制到用户空间的内存中） 但这样也有一个很明显的缺陷——不可靠，写到 mmap 中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用 flush 的时候才把数据真正的写到硬盘。所以 Kafka 提供了一个参数—— producer.type 来控制是不是主动 flush，如果Kafka 写入到 mmap 之后就立即 flush 然后再返回 Producer 叫同步(sync)；如果写入 mmap 之后立即返回，Producer 不调用 flush ，就叫异步(async)。\n\n### 3. 标准化二进制消息格式\n\n为了避免无效率的字节复制，尤其是在负载比较高的情况下影响是显著的。为了避免这种情况，Kafka 采用由 Producer，Broker 和 Consumer 共享的标准化二进制消息格式，这样数据块就可以在它们之间自由传输，无需转换，降低了字节复制的成本开销。\n\n而在读取速度的优化上 Kafak 采取的主要是零拷贝\n\n#### 零拷贝（Zero Copy）的技术：\n\n传统模式下我们从硬盘读取一个文件是这样的\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/acY0zH.jpg)\n\n(1) 操作系统将数据从磁盘读到内核空间的页缓存区\n\n(2) 应用将数据从内核空间读到用户空间的缓存中\n\n(3) 应用将数据写会内核空间的套接字缓存中\n\n(4)操作系统将数据从套接字缓存写到网卡缓存中，以便将数据经网络发出\n\n这样做明显是低效的，这里有四次拷贝，两次系统调用。 针对这种情况 Unix 操作系统提供了一个优化的路径，用于将数据从页缓存区传输到 socket。在 Linux 中，是通过 sendfile 系统调用来完成的。Java提供了访问这个系统调用的方法：FileChannel.transferTo API。这种方式只需要一次拷贝：操作系统将数据直接从页缓存发送到网络上，在这个优化的路径中，只有最后一步将数据拷贝到网卡缓存中是需要的。\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/M331Ew.jpg)\n\n这个技术其实非常普遍，The C10K problem 里面也有很详细的介绍，Nginx 也是用的这种技术，稍微搜一下就能找到很多资料。\n\n\n\nKafka 速度的秘诀在于它把所有的消息都变成一个的文件。通过 mmap 提高 I/O 的速度，写入数据的时候是末尾添加所以速度最优；读取数据的时候配合sendfile 直接暴力输出。所以单纯的去测试 MQ 的速度没有任何意义，Kafka 的这种暴力的做法已经脱了 MQ 的底裤，更像是一个暴力的数据传送器。","tags":["MQ","kafka"],"categories":["MQ","kafka"]},{"title":"kafka入门","url":"/2020/05/16/kafka1/","content":"# Kafka 入门\n\n## 消息队列\n\n### 什么是消息队列？\n\n​\t\t我们可以把消息队列比作是一个存放消息的容器，当我们需要使用消息的时候可以取出消息供自己使用。消息队列是分布式系统中重要的组件，使用消息队列主要是为了通过异步处理提高系统性能和削峰、降低系统耦合性。目前使用较多的消息队列有Kafka，ActiveMQ，RabbitMQ，RocketMQ。\n\n\n### 为什么要使用消息队列？\n\n我觉得使用消息队列主要有两点好处：\n\n1.  通过异步处理提高系统性能（削峰、减少响应所需时间）\n\n2.  降低系统耦合性\n\n#### (1) 通过异步处理提高系统性能\n\n​\t\t使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。\n\n​\t\t通过以上分析我们可以得出**消息队列具有很好的削峰作用的功能**——即**通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。** 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/T9CcMF.jpg)\n\n\n\n​\t\t因为**用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败**。因此使用消息队列进行异步处理之后，需要**适当修改业务流程进行配合**，比如**用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功**，以免交易纠纷。这就类似我们平时手机订火车票和电影票。\n\n#### (2) 降低系统耦合性\n\n​\t\t使用消息队列还可以降低系统耦合性。我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。\n\n​\t\t生产者（客户端）发送消息到消息队列中去，接受者（服务端）处理消息，需要消费的系统直接去消息队列取消息进行消费即可而不需要和其他系统有耦合， 这显然也提高了系统的扩展性。\n\n　　**消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。** 从上图可以看到**消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合**，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。**对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计**。\n\n### 不同消息队列的对比\n\n最关心的吞吐量数据的对比，根据阿里中间件团队的数据，在相同的测试机器上同步发送小消息(124字节)。\n\nKafka的吞吐量高达17.3w/s。这主要取决于它的队列模式保证了写磁盘的过程是线性IO。此时broker磁盘IO已达瓶颈。\n\nRocketMQ吞吐量在11.6w/s，磁盘IO 已接近100%。RocketMQ的消息写入内存后即返回ack，由单独的线程专门做刷盘的操作，所有的消息均是顺序写文件。\n\nRabbitMQ的吞吐量5.95w/s，CPU资源消耗较高。它支持AMQP协议，实现非常重量级，为了保证消息的可靠性在吞吐量上做了取舍。\n\n结合其它方面数据，我们得出一下表格：\n\n| 对比     | ActiveMQ | RabbitMQ         | RocketMQ       | kafka          |\n| -------- | -------- | ---------------- | -------------- | -------------- |\n| 吞吐量   | 万       | 万               | 十万           | 十万           |\n| 延时     | ms       | μs               | ms             | ms             |\n| 集群支持 | 主从     | 主从             | 分布式         | 分布式         |\n| 功能     | 极其完备 | 性能好，支持事务 | 较完善，分布式 | 简单、实时计算 |\n\n\n\n## Kafka 简介\n\n**Kafka 是一种分布式的，基于发布 / 订阅的消息系统。**\n\n对于 Kafka 来说客户端有两种基本类型：\n\n1. **生产者（Producer）**\n2. **消费者（Consumer）**。\n\n除此之外，还有用来做数据集成的 Kafka Connect API 和流式处理的 Kafka Streams 等高阶客户端，但这些高阶客户端底层仍然是生产者和消费者API，它们只不过是在上层做了封装。\n\n这很容易理解，生产者（也称为发布者）创建消息，而消费者（也称为订阅者）负责消费或者读取消息。\n\nKafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据\n\n### Topic 和 Partition\n\n![主题与分区](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/主题与分区.png)\n\n**Topic**\n\n每条发布到 Kafka 的消息都有一个类别，这个类别被称为 Topic 。（物理上不同 Topic 的消息分开存储。逻辑上一个 Topic 的消息虽然保存于一个或多个broker上，但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处）\n\n**Partition**\n\nTopic 物理上的分组，一个 Topic 可以分为多个 Partition ，每个 Partition 是一个有序的队列。Partition 中的每条消息都会被分配一个有序的 id（offset）\n\n### Broker 和 Cluster\n\n![Broker和集群](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/Broker和集群.png)\n\n**Broker**\n\n一个 Kafka 服务器也称为 Broker，它接受生产者发送的消息并存入磁盘；Broker 同时服务消费者拉取分区消息的请求，返回目前已经提交的消息。\n\n**Cluster**\n\n若干个 Broker 组成一个集群（Cluster），其中集群内某个 Broker 会成为集群控制器（Cluster Controller），它负责管理集群，包括分配分区到 Broker、监控 Broker 故障等。在集群内，一个分区由一个 Broker 负责，这个 Broker 也称为这个分区的 Leader；当然一个分区可以被复制到多个 Broker 上来实现冗余，这样当存在 Broker 故障时可以将其分区重新分配到其他 Broker 来负责。\n\n### Producer和Consumer设计\n\n#### Producer发送消息设计\n\n**1. 连接**\n\n每初始化一个producer实例，都会初始化一个Sender实例，新增到broker的长连接。\n\n**2. 计算partition**\n\n根据key和value的配置对消息进行序列化,然后计算partition： ProducerRecord对象中如果指定了partition，就使用这个partition。否则根据key和topic的partition数目取余，如果key也没有的话就随机生成一个counter，使用这个counter来和partition数目取余。这个counter每次使用的时候递增。\n\n**3. 发送到batchs**\n\n根据topic-partition获取对应的batchs（Deque），然后将消息append到batch中。如果有batch满了则唤醒Sender 线程。队列的操作是加锁执行，所以batch内消息时有序的。\n\n**4. Sender把消息有序发到 broker**\n\nKafka中每台broker都保存了kafka集群的metadata信息，metadata信息里包括了每个topic的所有partition的信息：leader、 leader_epoch、 controller_epoch、 isr、replicas等；Kafka客户端从任一broker都可以获取到需要的metadata信息；同时根据metadata更新策略（定期更新metadata.max.age.ms、失效检测，强制更新：检查到metadata失效以后，调用metadata.requestUpdate()强制更新。\n\n为实现Producer的幂等性，Kafka引入了Producer ID（即PID）和Sequence Number。对于每个PID，该Producer发送消息的每个<Topic, Partition>都对应一个单调递增的Sequence Number。同样，Broker端也会为每个<PID, Topic, Partition>维护一个序号，并且每Commit一条消息时将其对应序号递增。对于接收的每条消息，如果其序号比Broker维护的序号）大一，则Broker会接受它，否则将其丢弃。\n\n**5. Sender处理broker发来的produce response**\n\n一旦broker处理完Sender的produce请求，就会发送produce response给Sender，此时producer将执行我们为send（）设置的回调函数。至此producer的send执行完毕。\n\n\n\n#### Consumer消费消息设计\n\n**1. poll消息**\n\n- 消费者通过fetch线程拉消息（单线程）。\n- 消费者通过心跳线程来与broker发送心跳。超时会认为挂掉。\n- 每个consumer group在broker上都有一个coordnator来管理，消费者加入和退出，以及消费消息的位移都由coordnator处理。\n\n**2. 位移管理**\n\nconsumer的消息位移代表了当前group对topic-partition的消费进度，consumer宕机重启后可以继续从该offset开始消费。 在kafka0.8之前，位移信息存放在zookeeper上，由于zookeeper不适合高并发的读写，新版本Kafka把位移信息当成消息，发往\\__consumers_offsets 这个topic所在的broker，__consumers_offsets默认有50个分区。 消息的key 是groupId+topic_partition,value 是offset。\n\n**3. 重平衡**\n\n当一些原因导致consumer对partition消费不再均匀时，kafka会自动执行reblance，使得consumer对partition的消费再次平衡。 什么时候发生rebalance？\n\n- 组订阅topic数变更\n- topic partition数变更\n- consumer成员变更\n- consumer 加入群组或者离开群组的时候\n- consumer被检测为崩溃的时候\n\n当新的消费者加入消费组，它会消费一个或多个分区，而这些分区之前是由其他消费者负责的；另外，当消费者离开消费组（比如重启、宕机等）时，它所消费的分区会分配给其他分区。这种现象称为**重平衡（rebalance）**。重平衡是 Kafka 一个很重要的性质，这个性质保证了高可用和水平扩展。**不过也需要注意到，在重平衡期间，所有消费者都不能消费消息，因此会造成整个消费组短暂的不可用。**而且，将分区进行重平衡也会导致原来的消费者状态过期，从而导致消费者需要重新更新状态，这段期间也会降低消费性能。后面我们会讨论如何安全的进行重平衡以及如何尽可能避免。\n\n消费者通过定期发送心跳（hearbeat）到一个作为组协调者（group coordinator）的 broker 来保持在消费组内存活。这个 broker 不是固定的，每个消费组都可能不同。当消费者拉取消息或者提交时，便会发送心跳。\n\n如果消费者超过一定时间没有发送心跳，那么它的会话（session）就会过期，组协调者会认为该消费者已经宕机，然后触发重平衡。可以看到，从消费者宕机到会话过期是有一定时间的，这段时间内该消费者的分区都不能进行消息消费；通常情况下，我们可以进行优雅关闭，这样消费者会发送离开的消息到组协调者，这样组协调者可以立即进行重平衡而不需要等待会话过期。\n\n在 0.10.1 版本，Kafka 对心跳机制进行了修改，将发送心跳与拉取消息进行分离，这样使得发送心跳的频率不受拉取的频率影响。\n\n![消费者设计概要5](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/消费者设计概要5.png)\n\n**一些问题**\n\n**问题1:** Kafka 中一个 topic 中的消息是被打散分配在多个 Partition(分区) 中存储的， Consumer Group 在消费时需要从不同的 Partition 获取消息，那最终如何重建出 Topic 中消息的顺序呢？\n\n**答案：**没有办法。Kafka 只会保证在 Partition 内消息是有序的，而不管全局的情况。\n\n**问题2：**Partition 中的消息可以被（不同的 Consumer Group）多次消费，那 Partition中被消费的消息是何时删除的？ Partition 又是如何知道一个 Consumer Group 当前消费的位置呢？\n\n**答案：**无论消息是否被消费，除非消息到期 Partition 从不删除消息。例如设置保留时间为 2 天，则消息发布 2 天内任何 Group 都可以消费，2 天后，消息自动被删除。\n\n### Kafka高可用\n\nKafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据\n\n每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。\n\n如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的，如果这上面有某个 partition 的 leader，那么此时会从 follower 中重新选举一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。\n\n#### Isr\n\nKafka结合同步复制和异步复制，使用ISR（与Partition Leader保持同步的Replica列表）的方式在确保数据不丢失和吞吐率之间做了平衡。Producer只需把消息发送到Partition Leader，Leader将消息写入本地Log。\n\nFollower则从Leader pull数据。Follower在收到该消息向Leader发送ACK。一旦Leader收到了ISR中所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。这样如果leader挂了，只要Isr中有一个replica存活，就不会丢数据。\n\nLeader会跟踪ISR，如果ISR中一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（replica.lag.max.messages）或者Follower超过一定时间（replica.lag.time.max.ms）未向Leader发送fetch请求。\n\n\n\n#### 扩展\n\n1. 关于 Broker 的设计原理\n2. broker故障检查&&故障转移\n3. Kafka高吞吐量是如何实现\n\n**参考**\n\n[阿里中间件团队MQ对比](http://jm.taobao.org/2016/04/01/kafka-vs-rabbitmq-vs-rocketmq-message-send-performance/)\n\n[消息队列之Kafka](https://juejin.im/post/5a67f7e7f265da3e3c6c4f8b#heading-49)\n\n[Kafka系统设计开篇](kafka系统设计开篇)\n\n[消息队列其实很简单](https://github.com/Snailclimb/JavaGuide/blob/master/docs/system-design/data-communication/message-queue.md)\n\n","tags":["MQ","kafka"],"categories":["MQ","kafka"]}]